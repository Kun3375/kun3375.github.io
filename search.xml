<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MySQL 主从复制]]></title>
    <url>%2F2019%2F01%2FMySQL-%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%2F</url>
    <content type="text"><![CDATA[最近工作也比较忙，很久没有写新的日记了。开始重新整理一下知识点，先找一个有意思点开始吧。 复制原理这个原理其实十分简单，只要搜索一下，那些搜索引擎就会给出图示和合理的答案： 主库开启二进制日志（或者叫主机日志）bin-log。这会使得数据库在更新时记录下它的操作动作。 从机开启 slave 模式，使用一个 IO 线程去请求 master 的数据，记录在中继日志 relay-log 中；master 方面有一个 dump 线程配合传输这份数据。 从机的另一个 SQL 线程读取中级日志内容并解析后执行相应的操作。 复制方案通常的，我们做主从复制，不仅仅为了数据备份。同时会在从机上设置只读，对 MySQL 集群做读写分离，提高数据库的响(wu)应(jin)速(qi)度(yong)，所以一般都采用了异步复制方案。主机采用完全独立的 dump 线程来传输 bin-log，备份完全异步化。另一个方案是半同步复制。通过选择性开启半同步复制插件开启。主机在每次 bin-log 刷盘时通知 dump 线程传输数据并挂起，在完成给从机的传输后唤醒原线程返回客户端结果。这可以保证至少一个从机能获得完整的 bin-log 所对应 relay-log 信息，但是不能保证从机 SQL 执行完毕；同时，从机的卡顿、阻塞会影响主机的稳定。应用面不广。 主机参数首先需要修改主机的 MySQL 配置文件 my.cnf 或者（更推荐）更改自定义配置文件（需要在主配置文件中指定目录），一般在 /etc/mysql/conf.d/ 下。选几个主要参数说明一下： seriver-id=1服务端编号，值选个大于零的数就好，一般会用个 ip 啥的，没什么好说的，用来区别不同 MySQL 实例。 log-bin=/var/lib/mysql/binlog用来指定 bin-log 生成的路径和名称前缀，其实以上就是默认值和数据文件在一起，最后生成的 bin-log 是 /var/lib/mysql/binlog.xxxxxx。 binlog-format=mixed指定二进制信息的形式，有三种选项： row 拷贝命令结果 statement 拷贝命令，随机函数等命令无法精准复制 mixed 推荐，默认拷贝命令，不佳时拷贝结果 binlog-ignore-db=mysql指定需要忽略的库，不记录 bin-log。多个写多次。 binlog-do-db=db_business指定需要同步的库，以上二选一即可。多个写多次。 expire_logs_days=15可选，设置 bin-log 过期天数，到期自动删除。该值默认为 0，永不删除。可以使用 purge 命令手动删除 bin-log。也行。 slave_skip_errors=1062可选，设置忽略同步过程中的错误，比如 1062 是主键重复，1032 是数据不存在。 log_bin_trust_function_creators=true为真时，对函数和存储过程一样进行同步。 再讲两个额外的选项，本身和主从复制过程无关，但是会对集群和主从间的数据一致性造成影响： innodb_flush_log_at_trx_commit=1默认值：1。为了减少 IO 创造高效，在 innoDB 每次提交事务的时候，可以不立刻刷盘。设置 0 时：log-buf 每隔一秒同步 bin-log 并刷盘；设置 1 时：事务每次提交都会同步刷盘；2：每次同步 log 文件缓冲，但是每隔一秒刷盘。 sync_binlog=1默认值：1。设置多少次提交后进行刷盘，一般配合以上选项使用。这两个选项会对性能造成明显的影响。但是一般对数据一致性不敏感且追求速度的场合会进行调整。 从机参数从机同样修改配置文件。如果使用主机镜像或者拷贝的 docker volume 需要修改 /var/lib/mysql/auto.cnf 中的 UUID。 server-id=2类似主机配置。 binlog-do-db=db_business选择要复制的数据库。多个写多次。 binlog-ignore-db=mysql拒绝不复制的数据库。 relay_log=/var/lib/mysql/2-relay-bin可选，指定中继日志位置。，启动 slave 模式必然会产生中继日志，默认在 /数据目录/${host}-relay-bin.xxxxxx read-only=1从机只读，不要修改数据。 如果从机还作为主机的话： log-bin=secondary-binlog也需要开启 bin-log，其他主机参数也要配置哦。 log_slave_updates=1从机记录复制事件进二进制日志中。 操作流程搭建全新主从数据库环境先来说说搭建一个全新的主从架构。 主机创建数据同步用户并授权，尽量指定 ip，少给与权限尤其是 super 权限，super 权限用户可以无视 read-only…12grant replication slave, replication client on *.* to &apos;user_slave&apos;@&apos;%&apos; identified by &apos;Kun3375&apos;;flush privileges; 记录下主机 bin-log 位置信息 File / Position1show master status; 启动从机并设置主机信息，并启动 slave 状态1234change master to master_host=&apos;mysql-master&apos;, master_port=3306, master_user=&apos;user_slave&apos;, master_password=&apos;Kun3375&apos;, master_log_file=&apos;binlog.000014&apos;, master_log_pos=497;start slave; 检查 slave 状态信息。当 Slave_IO_Running 与 Slave_SQL_Running 同时为 Yes 意味着 IO/SQL 两线程正常工作，从机状态正常。1show slave status; 出现问题的同学可以检查一下各个配置，再次启动如果存在错误，尝试清除一下脏数据12stop slave;reset slave; 为既有服务器增加从机首先进行主库的备份操作，使用 mysqldump 命令（确保操作用户拥有权限）：1mysqldump -uUSERNAME -pPASSWORD --routines --single-transaction --master-data=2 --all-databases &gt; DUAMPFILE.sql 选项说明： –routines 同时导出存储过程和函数 master-data 默认值为 1。在备份文件追加 change master 主机指定命令。设置 2，可以将该语句追加并注释。该选项会自动打开 lock-all-tables 进行锁表，除非使用 single-transaction 见下文。 –single-transaction 开启单一事务备份。不在备份执行时进行锁表，而仅仅在开始时获取 master status 时候锁表（它依然隐含了以下几个短暂的动作）： FLUSH TABLES WITH READ LOCK SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ START TRANSACTION SHOW VARIABLES LIKE ‘gtid_mode’ SHOW MASTER STATUS UNLOCK TABLES –skip-lock-tables 尽管 single-transaction 这可以在不长时间锁表的情况下进行备份，并保证在相应 bin-log 位置下数据的准确。如果数据库在任何时候的访问都十分频繁，可能会无法执行 TABLE LOCK，如果可以接受可能有小部分数据不准确的风险，那么可以使用该参数来跳过获取 master status（即 bin-log 位置）前的锁表动作。– –all-databases 可以备份全库– –databases 用来指定需要备份的数据库，顺便整理一下导出语句的灵活用法123456# 导出多个库，主从备份时只能用这个或者 all-databasesmysqldump --databases DB_NAME_1 DB_NAME_2 &gt; DUMPFILE.sql# 导出一个库的多个表，不包含建库语句和 use 命令mysqldump DB_NAME TAB_NAME_1 TAB_NAME_2 &gt; DUMPFILE.sql# 导出结构不含数据mysqldump DB_NAME TAB_NAME_1 TAB_NAME_2 &gt; DUMPFILE.sql 将备份文件拷贝至从机，从机执行 source DUMPFILE.sql。如果之前 master-data 设置为 2 需要手动放开注释或者数据导入之后手动执行 change master 命令。最后 start slave 就好啦，记得使用 show slave status 确认从机状态。 为现有主从体系新增从机新增从机的我们可以在完全不对主库进行操作以减少风险。可以在从机上使用 mysqldump 并将 master-data 替换为 dump-slave，从从机导出数据并记录主机的 bin-log 位置。选项数值意义一致。需要注意从机在 dump 时候会暂停复制的 IO 线程。需要确保是否可以接受这样的同步延迟，考虑暂停某从机的使用。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES 数据类型、元字段与映射]]></title>
    <url>%2F2018%2F11%2FES-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%85%83%E5%AD%97%E6%AE%B5%E4%B8%8E%E6%98%A0%E5%B0%84%2F</url>
    <content type="text"><![CDATA[在 ES 中所有的字段都是由映射规则所控制，这将输入的数据信息转化成对应的数据格式来进行索引或保存。配置合理的映射规则有助于维护文档，增加操作效率。在了解映射相关配置之前需要了解一下 ES 的数据类型和元字段的意义。 字段类型 text文本类型，十分常用的类型，通常作用于需要被全文检索的字段上。这样的字段内容会被分词器拆成词项后生成倒排索引，它不用于排序，也很少用于聚合。 keyword关键字类型，通常用于索引结构化的字段（通常意义明确，用于过滤），这样的字段只能被精确搜索。 number数字类型，这是一个概括。其中包含了 byte，short，integer，long，float，double，half_float，scaled_float。除了和 Java 类似的数字类型以外，还有相对于 float 精度折半的 half_float，以及将浮点数进行缩放后存储的 scaled_float。字段长度越短，空间分配越合理，搜索效率越高。注意在浮点数中 +0.0 与 -0.0 是不同的存在。 12345678910111213141516# 设置某字段为 scaling_float，缩放因子 100# 适合存储精确至小数点后两位的数字，底层对数字扩大 100 做整形存储# 而对 API 为 float 型PUT /&lt;索引&gt;&#123; &quot;mappings&quot;: &#123; &quot;&lt;类型&gt;&quot;: &#123; &quot;properties&quot;: &#123; &quot;&lt;字段名称A&gt;&quot;: &#123; &quot;type&quot;: &quot;scaled_float&quot;, &quot;scaling_factor&quot;: 100 &#125; &#125; &#125; &#125;&#125; date日期类型，ES 支持日期格式化后的字符串、从 epoch 开始的毫秒数（长整型）、从 epoch 开始的秒数（整形），在 ES 内部，日期都会转化为 UTC 时间并存储为从 epoch 开始的毫秒数。在开启动态映射的时候如果有新的字段被添加，默认会自动进行日期检测以判断是否该字段为日期类型（可以被关闭，将某类型的 date_detection 选项设置为 false）。同时日期格式也支持自定义（通过制定字段的 format 选项，默认为 strict_date_optional_time || epoch_millis），除了 yyyy-MM-dd HH:mm:ss 这样的个性格式，其他预置的格式枚举很多，详情查看官方文档。 boolean布尔类型，只接受 true 和 false。 binary二进制类型，该类型字段仅接受 Base64 编码后的字符串，字段默认不存储（store=false）也不搜索。 array数组类型，其本身是其他类型。数组中的所有值必须为统一类型（可以包含 null），而空数组由于无法确定类型会被作为 missing field 对待。在动态映射时，第一个加入数组的元素会决定整个数组的数据类型。 object对象类型。在 JSON 中，对象是可以包含层级关系的，但是在 ES 中复合的对象会被扁平化处理，成为简单的 k-v 键值对。如果需要在建立索引时进行静态映射，mappings 支持 object 的显示映射。 nested嵌套对象，这是 object 类型的特例，支持 object 对象数据的独立索引和查询（ES 在使用对象类型的数组时由于扁平化处理会导致一些索引问题）。当指定了 nested 类型进行索引某个字段时，该字段会内容会作为独立的隐藏文档存在。这样支持了嵌套对象的索引，但是由于类似结构化数据的关联查询一般，nested 字段越多，搜索越复杂，所以每个索引可以使用嵌套对象被限制在 50。 geo_point地理坐标，用来精确存储地理经纬信息的类型，支持 4 中写入方式： 经纬坐标字符串，如：&quot;40.3,116.17&quot; 经纬坐标键值对，如：{&quot;lat&quot;: 40.3, &quot;lon&quot;: 116.17} 地理位置哈希值，如：&quot;drm3btev3e86&quot; 经纬坐标数组，如：[40.3, 116.17] geo_shape地理区块，使用 GeoJSON 对区块进行编码，描述一块地理区域，支持点线面等多种特征。一下列举集中典型表达，更多使用方案参数 GeoJSON 相关文档： GeoJSON 类型 ES 类型 说明 Point point 精确坐标点 LineString linestring 线条，多个点组成 Polygon polygon 封闭多边形，多个点组成 MultiPoint multipoint 多个不连续但可能关联的点 MultiLineString multilinestring 多条不关联的线 MultiPolygon MultiPolygon 多个不关联的多边形 GeometryCollection geometrycollection 集合对象集合，可以包括点线面 N/A envelope 由左上右下坐标确定的封闭矩形 N/A circle 圆心和半径确定的圆，单位米 在使用 geo_shape 类型之后，插入文档时指定字段必须明确 Geo 类型和数据，如：1234567PUT /&lt;索引&gt;/&lt;类型&gt;/&lt;编号&gt;&#123; &quot;&lt;geo_shape字段&gt;&quot;：&#123; &quot;type&quot;: &quot;linestring&quot;, &quot;coordinates&quot;: [[40.3, 116.17], [31.3, 116.17]] &#125;&#125; ipip 类型，可以保存 ip 地址，支持 IPv4 及 IPv6，以及无类型域间选路格式 range范围类型，支持 integer_range，long_range，float_range，double_range，date_rage。其中日期区间以毫秒计时。在某字段使用 rage 类型之后，插入数据需要指定对应的范围，可以使用 gt、lte 等关键字描述。 token_count词项统计类型，其本身是一个整形。一般用来给某个属性增加附加字段并指定 token_count 来统计词项长度。词项长度取决于具体内容和指定的分词器。 元字段元字段描述了文件本身的属性，是 ES 内置的。总的来看元字段描述了从文档属性、源文档、索引、路由等相关信息，同时也支持自定义元字段。这些元字段支持部分的查询方式和脚本。 元字段 元字段分类 意义 _index 文档属性 描述文档所属的索引 _type 文档属性 描述文档的类型 _id 文档属性 描述文档的编号 _uid 文档属性 包含_type及_id _source 源文档属性 文档原始的JSON资料 _size 源文档属性 描述源文档的大小，需要插件 mapper-size _all 索引属性 包含索引中全部的字段的内容，用来泛检索 _field_names 索引属性 包含所有不存在空值的字段名 _parent 路由属性 指定文档间的父子关系 _routing 路由属性 用来设定文档进行路由的自定义值 _meta 自定义 自定义的元数据 映射参数在设置索引的映射时候，有一些针对索引、类型或者具体文档属性的参数可以选择性调整。语法：12345678910111213PUT /index_name&#123; &quot;mappings&quot;: &#123; &quot;type_name&quot;: &#123; &quot;&lt;类型配置如 dynamic 等&gt;&quot;: &lt;配置内容&gt;, &quot;properties&quot;: &#123; &quot;property_name_one&quot;: &#123; &quot;&lt;属性配置如 boot 等&gt;&quot;: &lt;配置内容&gt; &#125; &#125; &#125; &#125;&#125; 以上的命令也可以简化使用 _mapping API：12345678PUT /index_name/_mapping/type_name&#123; &quot;properties&quot;: &#123; &quot;property_name_one&quot;: &#123; &quot;&lt;属性配置如 boot 等&gt;&quot;: &lt;配置内容&gt; &#125; &#125;&#125; 接下来是一些可配置项： analyzer分词器选项，针对文档属性，调整对应字段的默认分词器，会影响文档的索引以及查询（未指定 search_analyzer 时）。 search_ananlyzer查询分词器选项，针对文档属性，仅查询生效，可以覆盖 analyzer 选项。 normalizer标准化配置，针对文档属性，用于属性值在解析前的预处理。对某属性使用的标准化配置需要在设置时配置好。 boost权重，针对文档属性，默认值为 1，可以手动通过该选择项改变关键字在某属性中出现时的权重。但是在映射配置中设定 boost 后如果不重新索引文档是无法改变权重的，所以更推荐在搜索时指定权重，更为灵活且效果一样。 coerce强制转型，针对文档属性，默认值 true，用于将类型不正确的输入数据自动转化为文档中对应的类型。 copy_to字段拷贝，用于自定义 _all 字段，可以将多个字段内容拷贝进指定的字段。 doc_value建立倒排索引时的额外的列式存储映射开关，针对文档属性，默认值 true，牺牲空间换取排序、聚合操作的速度。如果明确一些字段不需要排序或者聚合操作可以关闭。 dynamic新字段自动映射开关，针对类型。在插入文档时如果文档中含有没有指定配置过的属性，插入的结果会取决于该选项的设置。它有三个可选项，默认为 true： true 对新增的字段采取自动映射 false 忽略未映射的新字段 strict 严格模式，如果发现新字段会抛出异常 enableES 默认索引所有字段，但是某些字段没有查询、聚合等需求，可以直接使用 &quot;enable&quot;: false 来直接关闭。关闭的字段不会被索引和搜索，需要获取值时可以从 _source 中得到。 fielddata这是一个特殊的选项。上文可知大部分类型字段默认都会生成 doc_value 以加快排序和聚合，但是 text 类型除外，取而代之的是在 text 首次被排序、聚合或者使用脚本时生成 fielddata 数据。fielddata 是在堆内存中的记录文档和词项关系的数据，所以它非常消耗内存，默认是不开启的。因为大部分情况下对一个 text 字段做排序聚合似乎都是无意义的。 format针对日期字段设定格式 ignore_above针对 keyword 类型的属性，如果目标字段的内容长度超过设定值，将不会被索引（查询不到哦） ignore_malformed针对文档属性，支持不兼容数据类型的开关，打开时，如果某个字段存在不兼容数据类型，异常会被忽略，仅该属性被忽略，其他字段仍然索引，可以避免存在不规则数据时整个文档索引出错。 include_in_all针对文档属性，每个字段的该选择项默认为 true，即所有字段内容都会加入 _all，如果需要 _all 中不包含某字段可以设置为 false。 index设定某个字段是否被索引，如果关闭了当然就不能被搜索了 index_options针对文档属性，控制某属性被索引时保存进倒排索引表中的信息，具体取值有下： docs 默认，只保存文档编号 freqs 保存文档编号和词项频率 positions 保存文档编号、词项频率和词项偏移位置（用于临近搜索和短语查询） offset 保存文档编号、词项频率、词项位置、词项开始和结束字符位置。 fields针对文档属性，可以为某个属性增加附加的属性，以使用额外的索引方式或者 token_count。 norms标准化文档，针对某个文档属性，用于文档评分，会占用额外的磁盘空间，如果不需要评分可以关闭。 null_value空值映射，针对文档属性，通常值为 null 的字段不会被索引也不能被搜索，这时候可以显式地告诉 ES null 值需要映射成什么，如：&quot;null_value&quot;: &quot;NULL&quot; 会使得某个字段的空值显式地转化为 NULL 字符串，以被索引和查询。 position_increment_gap一般针对某 text 数组类型属性，因为 text 类型在索引时会考虑词项的位置，而对于一个 text 数组，其中每个元素在保存的时候会有一定的 间距 ，通过这个间距（默认 100）来区分不同元素。举一个 match_phrase query 的例子：123456789101112131415PUT /player/football/1&#123;&quot;name&quot;: [&quot;Lionel Messi&quot;, &quot;Cristiano Ronaldo&quot;]&#125;GET /player/football&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;name&quot;: &#123; &quot;query&quot;: &quot;Messi Cristiano&quot;, # 为了查询到该文档，需要 &quot;slop&quot;: 101 &#125; &#125; &#125;&#125; 像上面了例子也可以改变字段的的间距值，比如：&quot;position_increment_gap&quot;: 0。 properties这个选项其实用的太普遍了以至于我们都忽略了。如果把 properties 看作一个配置项，那么它是针对某个类型的，用来指定属性的类型和其他配置。 similarity用于指定某字段会用的评分模型，ES 中预置了三种模型： BM25 默认评分模型 classic TF/IDF 评分模型 boolean 布尔评分模型 store决定某个字段是否被存储。默认字段会被索引但是不会存储，因为 _source 中包含了源文档的数据。 term_vector决定词项量存储时候包含的信息： no 默认值，不存储词向量 yes 保存词项集合 with_positions 保存词项和词项位置 with_offsets 保存词项和字符偏移位置 with_positions_offsets 保存词项、词项位置和字符偏移位置 dynamic_templates这也是一个特殊配置项，针对某个类型而言，配置 dynamic template 可以在字段进行自动映射时候按一定的规则确定索引字段的类型及别的配置。模板中至少需要包含一个条件，多个模板存在先后关系，最先匹配的模板会被应用。下面是一个例子：当文档中添加以 long_ 开头而不以 _text 结尾的字段时自动映射为 long 数据类型：12345678910111213141516171819PUT /index_name&#123; &quot;mappings&quot;: &#123; &quot;type_name&quot;: &#123; &quot;dynamic_templates&quot;: [ &#123; &quot;template_name&quot;: &#123; &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;match&quot;: &quot;long_&quot;, &quot;unmatch&quot;: &quot;_text&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; ] &#125; &#125;&#125; 以上内容涉及了字段类型、元字段性质以及配置映射时候的选项，本质是对索引管理内容的深化，除了了解 ES 本身的机制，这些内容的学习可以为学习 ES 带来更好的铺垫。 参考： 《从Lucene到ElasticSearch全文检索实战》 姚攀]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES 分片路由]]></title>
    <url>%2F2018%2F10%2FES-%E5%88%86%E7%89%87%E8%B7%AF%E7%94%B1%2F</url>
    <content type="text"><![CDATA[分片路由ES 在创建新文档时候时如何选择具体存储在哪个分片上？这就是一个文档分片路由的机制。ES 使用路由值（routing）的哈希散列进行分片的路由：index_of_shards = hash(routing) % num_of_shards。 ES 查询请求过程假设在一个多分片的 ES 进行查询，会有以下几个步骤： 请求被某个节点接受 接受请求的节点将该请求广播到所有节点 每个分片进行搜索，并返回 各个分片的结果在一个节点合并，排序，返回响应 利用路由值进行搜索优化在默认情况下路由值等于文档的编号：routing=_id，这样可以让文档均匀的散列在所有分片上。但是在很多查询的时候 ES 无法确定查询的文档所在分片，所有会向所有分片进行广播。指定路由值可以避免广播请求，一定程度上减少资源消耗和增加效率，在请求时跟上 routing 参数即可：1GET /index/type/_search?routing=123123 指定 routing 的这个时候请求指向的分片事实上是确定的！这样的请求通常是查询的文档已经通过同样的路由值被添加了！否则这样的查询是无效的。 路由值是可以指定多个的，中间使用逗号 , 隔开。 显式路由指定的弊端通常各个分片上的数据是均匀分布的（默认使用 id 进行哈希），但是如果大量使用显式路由指定文档，容易造成数据偏移在某些节点上。所以使用时需要对数据有足够了解。 参考： 《从Lucene到Elasticsearch全文检索实战》 姚攀]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES 操作-文档管理]]></title>
    <url>%2F2018%2F10%2FES-%E6%93%8D%E4%BD%9C-%E6%96%87%E6%A1%A3%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[创建文档ES 提供了一套遵守 Restful 语义的接口，相关的操作可以通过不同的请求方法来进行调用，比如简单的新增可以使用 POST 请求或者 PUT 请求123456789101112# PUT 多次请求结果幂等，仅增加文档版本号，文档编号不可省略PUT /&lt;索引&gt;/&lt;类型&gt;/&lt;编号&gt;&#123; "字段A": "值A", "字段B": "值B"&#125;# POST 请求在版本号省略时，可以自动生成字符串文档编号POST /&lt;索引&gt;/&lt;类型&gt;/[编号]&#123; "字段A": "值A", "字段B": "值B"&#125; 简单查询单一文档指定查询查询特定的文档，（先不考虑条件查询等）需要明确指定索引名称，类型名称和文档编号，如果文档存在，返回的 found 域会为 true1234# 使用 GET 请求获取指定的文档信息GET /&lt;索引名称&gt;/&lt;类型名称&gt;/&lt;文档编号&gt;# 可以使用利用版本号-version来控制资源的时效性（ES 悲观锁机制），版本号不正确会抛出异常GET /&lt;索引名称&gt;/&lt;类型名称&gt;/&lt;文档编号&gt;?version=&lt;期望版本号&gt; 如果不需要文档内容，只需要确认文档是否存在可以使用 HEAD 请求12# HEAD 请求，200 成功，404 未找到HEAD /&lt;索引名称&gt;/&lt;类型名称&gt;/&lt;文档编号&gt; 多个文档指定查询在指定查询多个文档时候需要使用 _mget API：123456789101112131415GET /[公共索引名称]/[公共类型名称]/_mget&#123; "docs": [ &#123; "_index": "具体索引名称A", # 当未指定公共索引名称时需要 "_type": "具体类型名称A", # 当未指定公共类型名称时需要 "_id": "具体文档编号" &#125;, &#123; "_index": "具体索引名称B", "_type": "具体类型名称B", "_id": "具体文档编号" &#125; ]&#125; 在查询同一索引同一类型时最简写法：1234GET /&lt;公共索引名称&gt;/&lt;公共类型名称&gt;/_mget&#123; "ids": ["编号1", "编号2", "编号3"]&#125; 文档更新简单更新操作文档更新时候，ES 会删除旧的文档，更新文档内容后索引新的文档。通常，最简单的更新操作可以直接使用 PUT 操作完成：12345PUT /&lt;索引&gt;/&lt;类型&gt;/&lt;编号&gt;&#123; "字段A": "新值A", "新字段B": "新值B"&#125; ES 悲观锁机制，默认情况下每次更新会使得版本号增 1，其实也可以手动指定值（当然指定的版本号需要大于当前版本号）12345PUT /&lt;索引&gt;/&lt;类型&gt;/&lt;编号&gt;?version=&lt;版本号&gt;&amp;version_type=external&#123; "字段A": "新值A", "新字段B": "新值B"&#125; 脚本更新操作更为复杂的更新操作可以使用 POST 请求操作 _update API，下面是一个简单例子：1234567891011121314151617181920POST /&lt;索引&gt;/&lt;类型&gt;/&lt;编号&gt;/_update&#123; # 声明使用脚本进行操作文档 "script": &#123; # 书写脚本内容（the 'inline' is deprecated） # 这里五个语句分别是：字段值修改，移除字段，新增字段（类似修改），字段值修改（api调用），文档删除 "source": "ctx._source.count += params.count; ctx._source.remove(\"to_be_removed\"); ctx._source.tags=[]; ctx._source.tags.add(params.class); ctx._source.op=\"delete\"", # 声明脚本语言 "lang": "painless", # 输入脚本入参集合 "params": &#123; "count": 3, "class": "Java" &#125; &#125;, # 可选，当指定更新的资源不存在时，进行插入 "upsert": &#123; "字段A": "值A" &#125;&#125; 条件过滤更新同时 ES 也支持条件查询后对文档进行更新，使用 _update_by_query，类似 SQL 中 update…where 的形式123456789101112POST /[索引名]/[类型名]/_update_by_query&#123; # 脚本的具体使用同上 "script": &#123; &#125;, # 查询条件，具体之后深入 "query": &#123; "term": &#123; "age": 18 &#125; &#125;&#125; 删除操作简单删除操作很简单的，使用 DELETE 方法并指定资源即可：1234# 删除指定文档，自动路由DELETE /&lt;索引&gt;/&lt;类型&gt;/&lt;文档编号&gt;# 删除指定文档，并给定路由参数DELETE /&lt;索引&gt;/&lt;类型&gt;/&lt;文档编号&gt;?routing=&lt;路由值&gt; 条件过滤删除如同更新操作，可以在删除前进行查询操作，以过滤出需要删除的文档，使用 _delete_by_query API 操作：123456POST /[索引名]/[类型名]/_delete_by_query&#123; "query": &#123; # 查询条件 &#125;&#125; 批量操作很多时候需要同时操作大量的文档，一个一个来执行命令显然是不可能的。ES 提供了 _bulk API，并支持多文档的创建，更新，删除等操作，但是对格式有一定的要求。首先需要一个 JSON 格式的内容：12345678910# 新建&#123;"create / index": &#123;"_index": "索引名称", "_type": "类型名称", "_id": "不指定则自动生成"&#125;&#125;&#123;&lt;文档内容&gt;&#125;# 删除&#123;"delete": &#123;"_index": "索引名称", "_type": "类型名称", "_id": "id值"&#125;&#125;# 更新&#123;"update": &#123;"_index": "索引名称", "_type": "类型名称", "_id": "id值", "_retry_on_conflict": 3&#125;&#125;&#123;"doc": &#123;&lt;需要更新的内容&gt;&#125;&#125; 然后使用一个 POST 请求：1curl -XPOST "地址/_bulk?pretty" --data-binary @JSON文件名 bulk 请求时注意文件的大小，因为整个请求会被加载进被请求的节点，所以同时可供其他请求的内存会相应变小。合适的大小值不是一个固定值，这取决机器配置和索引复杂度，搜索负载等。一个合适的批次通常在 5~15 MB之间。 参考： 《从Lucene到Elasticsearch全文检索实战》 姚攀]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 命令笔记]]></title>
    <url>%2F2018%2F10%2FGit-%E5%91%BD%E4%BB%A4%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[初始化仓库 1234# 初始化一个 git 仓库并且建立工作目录git init# 初始化一个干净的仓库，使用 bare 选项不带有工作目录，使用 shared 选项，提供组可写的权限git init --bare --shared 克隆仓库 12# 在未指定协议的时候优先会采用 sshgit clone [地址] 暂存文件将文件提交至暂存区（进入 staged 状态） 123git add [路径]# 只暂存所有已经跟踪的文件，这通常可以减少多余文件的提交git add -u 查看当前工作区以及暂存区的文件状态 12# 该命令会展示所有文件的状态，包括它是否已经修改，是否被跟踪，是否被暂存git status 查看工作区和暂存区文件差异 1234567891011# 查看工作区和暂存区快照间的差异（文件与上一次 add 时的差异）git diff# 查看工作区和上次提交时的差异（文件与上一次 commit 时的差异）git diff --stagedgit diff --cached# 可以携带版本号，可以指定文件名产看，版本间指定文件的差异# 第二版本号省略时默认为当前版本，文件路径省略时默认所有文件git diff [版本号] [版本号]git diff [版本号] [版本号] -- [文件路径]# 查看在之前祖先基础上的差异需要在源版本号前添加'...'，如：git diff master...feature 提交更新 123456# 在暂存区准备妥当之后，把暂存区内容提交git commit# 在提交时会要求输入提交备注，可以直接在命令行键入git commit -m [提交备注]# 偶尔跳过暂存步骤，将工作区全部的修改直接提交git commit -a 文件删除 123456# 删除动作会将文件删除，并且在完成动作时修改暂存区git rm [路径]# 如果该文件已经被本次修改后暂存（staged）需要强制删除git rm -f [路径]# 从仓库中删除而不删除工作区文件，即放弃跟踪git rm --cached [路径] 文件移动 12# 如同文件删除，移动命令会在移动后对暂存区修改git mv [源路径] [目标路径] 查看提交历史 123456789101112131415161718192021222324252627# 查看历史时会展示历次提交的哈希值、作者、时间以及提交备注git log# 使用 p 选项展示内容差异git log -p# 可以指定展示条目数量git log -[N: int]# 展示增删改行数统计git log --stat# 按单词维度检测修改内容，需要同时指定显示的上下行数git log --word-diff -U[N: int]# 支持图形化展示git log --graph# 支持格式化记录的显示git log --pretty=[FormatMode]# FormatMode 常用形式# oneline 单行显示，仅显示提交的哈希值和备注# fuller 完整显示，包括作者，创建时间，提交者，提交时间，提交备注等# format:[格式字符串] 按指定格式输出，具体占位符含义查看手册# 查看在 A 分支上而不在 B 分支上的提交，有多种写法git log B..Agit log ^B Agit log A --not B# 查看在多个分支上仅不在 B 分支上的提交git log A C --not Bgit log A C ^B# 查看在 A 或 B 上，但不同时存在于 A 和 B 上的提交，使用 left-right 选项区分提交所在分支位置git log --left-right A...B 文件修改 提交修改 12345# 如果遗漏部分提交，可以重新更新暂存区后使用 amend 选项修改之前次的提交，比如如下操作git commit -a -m 'first commit'touch a.txtgit add a.txtgit commit --amend -m 'first commit with a.txt' 撤销暂存区快照 12# 取消暂存状态的修改git reset HEAD [路径] 撤销工作区修改 12# 这会使工作区的内容还原至上一次提交时的状态，慎用git checkout -- [路径] 还原代码至指定的版本（也可以向上） 1234# mixed 默认选项，回退 HEAD 并还原暂存区，所有修改保留在工作区# soft 选项，仅回退 HEAD，保留暂存区和工作区的改动# hard 选项，将回退 HEAD 并还原暂存区和工作区内容，彻底的回退git reset [--soft/hard/mixed] [版本号] 远程仓库 12345678910# 远程仓库列表，默认仅显示仓库名称，v 选项列出具体地址git remote -v# 添加远程仓库git remote add [远端名称] [远端仓库地址]# 展示远端具体信息，包括未跟踪的分支、提交的信息等git remote show [远端名称]# 重命名远程名称git remote rename [源远程名称] [新远程名称]# 删除关联的远程仓库git remote rm [远程名称] 标签处理 123456789101112# 展示所有标签，git tag# 使用 l 选项过滤标签，可以使用通配符git tag -l 'v1.*'# 创建轻量级标签，如果版本哈希值省略，则为当前版本打标签git tag [标签名称] [版本哈希值或分支名或标签名]# 创建携带附注的标签，使用 m 选项可以直接输入标签备注git tag -a [标签名称] -m [标签备注]# 使用 GPG 签名创建标签git tag -s [标签名称]# 验证标签，仅对于签名的标签git tag -v [标签名称] 分支处理 123456789101112# 查看所有分支，默认展示本地分支，使用 r 选项展示远端分支，a 选项全部展示git branch -a# 新建分支，如果版本哈希值省略，默认在当前版本新建分支git branch [分支名称] [版本哈希值或分支名或标签名]# 新建分支后并不会直接切换至该分支，需要进行切换git checkout [分支名称]# 新建并检出一个分支git checkout -b [新分支名称] [版本哈希值或分支名或标签名]# 使用 track 选项简化上述命令，直接新建同名分支并跟踪远端分支git checkout --track [远程名]/[分支名称]# 使用 d 选项删除分支git branch -d [分支名称] 拉取和推送 1234567# 大部分时候，远程名称可以省略，默认使用 origin# 拉取远程仓库的数据（这不会直接进行合并git fetch [远程名称]# 向远端推送数据，远程分支名称省略时推送至已关联的远程分支上，本地分支名称和远程分支名称一起省略时可以推送当前分支至远程对应分支上git push [远程名称] [本地分支名称]:[远程分支名称]# 删除远程分支git push [远程名称] :[远程分支名称] 变基操作 123456789# 可以将某一些特性在指定的基分支上重演，省略特性分支时默认为当前分支git rebase [基分支] [特性分支]# 有时候需要变基的特性分支和并不直接基于基分支，在需要跳过中间分支的情况下需要使用 onto 选项git rebase --onto [基分支] [跳过分支] [特性分支]# 变基过程中可能遇到冲突，git 会自动停下等待处理# 冲突处理完成后继续git rebase --continue# 放弃处理冲突，放弃变基git rebase --abort 使用补丁 1234567891011121314151617181920# 使用 diff 创建一个简单的补丁，diff 的输出就是一个标准的 patch 内容，例子如下：git diff master &gt;&gt; [补丁路径]# 打上补丁，之后需要手动提交git apply [补丁路径]# 添加补丁前最好测试是否存在冲突git apply --check [补丁路径]# 使用 git format-patch 创建补丁，该补丁已邮件方式存在，并包含提交者信息，可以添加额外的描述，通常使用 M 选项检测重命名git format-patch [比较的版本号]git format-patch HEAD^^# 使用 format-patch 创建的补丁需要使用 am 指定来确认git am [补丁路径]# 如同变基操作一样，在遇到冲突或者无法快速合并时候，git 会停下让你解决冲突或者放弃# 冲突解决完成，继续git am --[continue/resolved/r]# 放弃并还原git am --abort# 跳过该补丁git am --skip# 在存在公共祖先的时候，git 可以更智能地合并git am -3 [补丁路径] 发布 1234567# 发布前准备，需要打上一个标注标签（使用 a 或 s 选项）# 查看内部版本号，通常用来做归档压缩包的名称git describe [分支或标签名称]# 进行归档，可以使用 prefix 增加根目录，使用 format 指定格式git archive [分支或标签名称] --prefix='[增加的根目录名称/]' --format=[zip/tar/tar.gz/...]# 上述命令仅输出二进制内容，需要将其输入至归档文件，如git archive master --prefix='project/' --format=zip &gt; `git describe master`.zip 储藏内容git 可以帮助储藏一份临时性的改动便于之后恢复 1234567891011121314151617# 将已跟踪的文件的改动进行储藏git stashgit stash push [描述]# 查看储藏栈内所有的储藏记录git stash list# 将储藏内容还原至工作区和暂存区，名称省略时使用栈顶记录git stash apply [stash@&#123;N&#125;]# 删除储藏栈中的记录git stash drop stash@&#123;N&#125;# 清除全部储藏站中的记录git stash clear# 将储藏内容还原，并立即移除记录git stash pop stash@&#123;N&#125;# 还原应用的储藏内容，通过 show -p 选项展示 diff patch 内容，并使用 apply -R 来撤销应用git stash show -p [stash@&#123;N&#125;] | git apply -R# 直接从储藏中新开分支来避开冲突解决，使用栈顶记录时可省略储藏名称，完成时会自动移除储藏栈记录git stash branch [新分支名称] [stash@&#123;N&#125;]]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES 操作-索引管理]]></title>
    <url>%2F2018%2F09%2FES-%E6%93%8D%E4%BD%9C-%E7%B4%A2%E5%BC%95%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[新建索引新建索引很简单，但是需要注意的是： ES 中索引名称不能包含大写字母 不能再次 PUT 一个已经存在的索引 ES 默认给索引设置 5 个分片和 1 个副本，该值可以通过 setting 参数域进行修改。其中副本数在索引创建之后支持修改，而分片数无法修改！12345678910PUT /person# 可选项，不附加请求体的情况下统一使用默认值&#123; "settings": &#123; # 分片数量 "number_of_shards": 3, # 副本数量 "number_of_replicas": 1 &#125;&#125; 更新索引对某属性设置相应的值即可。如果设置为 null 可以将其恢复成默认值1234PUT /person/_settings&#123; "number_of_replicas": 2&#125; 索引设置部分设置的含义在后文涉及 静态设置这部分的设置只有在索引创建或者关闭时支持修改（分片数量只能在创建索引时设置） 123456789101112# 主分片数，默认为5.只能在创建索引时设置，不能修改index.number_of_shards# 是否应在索引打开前检查分片是否损坏，当检查到分片损坏将禁止分片被打开# false //默认值# checksum //检查物理损坏# true //检查物理和逻辑损坏，这将消耗大量内存和CPU# fix //检查物理和逻辑损坏。有损坏的分片将被集群自动删除，这可能导致数据丢失index.shard.check_on_startup# 自定义路由值可以转发的目的分片数。默认为 1，只能在索引创建时设置。此值必须小于index.number_of_shardsindex.routing_partition_size# 数据压缩方式，默认使用LZ4，也可以设置为 best_compression，它使用 DEFLATE 方式以牺牲字段存储性能为代价来获得更高的压缩比例index.codec 动态设置这部分配置支持直接修改 1234567891011121314151617181920# 每个主分片的副本数，默认为 1index.number_of_replicas# 基于可用节点的数量自动分配副本数量，默认为 false（即禁用此功能）index.auto_expand_replicas# 执行刷新操作的频率，这使得索引的最近更改可以被搜索。默认为 1s。可以设置为 -1 以禁用刷新。index.refresh_interval# 用于索引搜索的 from + size 的最大值，默认为 10000index.max_result_window# 在搜索此索引中 rescore 的 window_size 的最大值index.max_rescore_window# 设置为 true 使索引和索引元数据为只读，默认 false 为允许写入和元数据更改index.blocks.read_only# 设置为 true 可禁用对索引的读取操作，默认 falseindex.blocks.read# 设置为 true 可禁用对索引的写入操作，默认 falseindex.blocks.write# 设置为 true 可禁用索引元数据的读取和写入，默认 falseindex.blocks.metadata# 索引的每个分片上可用的最大刷新侦听器数index.max_refresh_listeners 查询索引关于查询也是简单的，通过 GET 请求和 _setting API 可以获得索引的配置信息。而 _cat API 可以以摘要的形式展示所有索引的开关状态，健康状态，分片数，副本数以及一些其他信息12345678910# 查询单个索引设置GET /person/_settings# 查询多个索引设置GET /person,animal/_settings# 按通配符查询索引设置GET /p*/_settings# 查询所有索引设置GET /_all/settings# 使用 _cat API 来展示所有索引的综合信息GET /_cat/indices 删除索引删除是最简单，当然如果指定的索引名称不存在会响应 4041DELETE /person 索引开关可以关闭一些暂时不用的索引来减少系统资源的开销，关闭后将无法进行读写1POST person/_close 相对的，打开操作：1POST person/_open 这里依然支持同时操作多个索引，以及通配符和 _all 关键字的处理：1POST /person,animal/_close 如果同时指定的索引中存在不存在的索引，会显示抛出错误。这可以通过一个简单参数 ignore_unavailable=true 来忽略1POST /person,animal/_close?ignore_unavailable=true 索引复制通过 _reindex API 可以将一个索引的内容复制至另一个索引，这同时可以指定过滤条件，已筛选需要的 type 以及 doc。需要注意的是，由于这不会同时复制索引的配置信息，所以在操作的时候需要提前建立好新索引123456789101112131415POST /_reindex&#123; "source": &#123; "index": "person", # 可选项，用于过滤类型 type "type": "student" # 可选项，用于过滤文档 doc "query": &#123; "term": &#123; "sex": "male" &#125; &#125; &#125;, "dest": &#123; "index": "person_new" &#125;&#125; 索引收缩分片数量在索引初始化之后便无法修改。_shrink API 可以将一个索引收缩成一个分片数量更少的索引。当然，这是有要求的： 收缩后索引的分片数必须是收缩前索引的分片数的因子，如 8 -&gt; 4，或者 15 -&gt; 5。这意味着如果源索引的分片数如果是质数，那么很尴尬，只能收缩成 1 个分片的新索引 收缩前，索引的每个分片需要都存在于同一节点上（可以指定路由实现） 索引必须为只读状态 后两者条件可以通过以下指令实现：12345PUT /person/_settings&#123; "index.routing.allocation.require._name": "shrink_node_name", "index.block.write": true&#125; 接下来便可以实际进行索引的收缩了，ES 完成的流程包括这几个步骤： 创建一个配置与源索引相同但是分片数减少的新索引 源索引硬链接至新索引（文件系统不支持的情况下会进行复制） 打开新的索引 _shrink 的一个例子如下：123456789101112POST /person/_shrink/person_new&#123; "settings": &#123; "index.number_of_replicas": 0, "index.number_of_shards": 1, "index.codec": "best_compression" &#125;, # 就如同新建索引一样，可以同时设置别名 "aliases": &#123; "pn": &#123;&#125; &#125;&#125; 索引别名通过 _aliases API 可以选择为索引设置别名，这就像软连接一样12345678910111213141516171819POST /_aliases&#123; "actions": [&#123; # 新增索引，可以一次操作多个索引，并合并书写 "add": &#123; "indices": ["animal", "plant"], "alias": "living_thing" &#125;&#125;, &#123; # 删除索引，也可以一次操作多个索引，支持通配符 "remove": &#123; "index": "person", "alias": "person_alias" &#125;&#125;, &#123; "remove": &#123; "index": "school", "alias": "school_alias" &#125;&#125; ]&#125; 在通常使用时，别名基本和原索引名用法一样。但是如果别名和索引不是一对一关系的时候，无法通过别名索引文档或者通过 ID 来查询 关于索引别名的查询也十分简单：123456# 如果想知道某索引（如 person）的别名GET /person/_aliases# 获取所有别名GET /_aliases# 使用 _cat API 获取所有索引摘要GET /_cat/aliases 补充 Restful 语义在 ES 操作中，以下几个请求方式是最常用的，补充一下以下请求代表的语义： GET：获取资源信息 DELETE：删除指定的资源标识符下的资源 POST：提交一个新的资源，不具备幂等性 PUT：新增或更新一个资源标识符下资源，操作具有幂等性 HEAD：获取该次请求的响应头信息]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ 服务端消息过滤]]></title>
    <url>%2F2018%2F08%2FRocketMQ-%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%B6%88%E6%81%AF%E8%BF%87%E6%BB%A4%2F</url>
    <content type="text"><![CDATA[在服务端进行消息过滤，可以减少不必要的流量，提高带宽利用度和吞吐量。RocketMQ 支持多种方式来进行服务端的消息过滤 消息使用 Tag 标签作为一条 Message，它有着特定的 Topic，同时也可以指定唯一的 Tag 标记子分类。消费方在订阅消息时候，Broker 可以在指定 Topic 的 ConsumeQueue 下按 Tag 进行过滤，只从 CommitLog 中取出 Tag 命中的消息。使用 Tag 进行过滤是高效的，因为消息在 MessageQueue 的存储格式如下： CommitLog Offset：顾名思义，保存着在 CommitLog 中的偏移量，占用 8 个字节 Size：使用 4 个字节来记录消息的大小 Message Tag HashCode：记录对应消息的 Tag 的哈希 在获取消息时候，通过 Tag HashCode 的对比，从 CommitLog 读取对应消息。由于哈希冲突实际上是不可避免的，消息在从 CommitLog 中拉取之后被消费之前，仍然会进行 Tag 的完整对比，以消除潜在哈希冲突问题 携带 MessageKey 来发送和查询其实这部分内容并不属于服务端消息过滤的功能，但是也为我们提供了一种较精确的查询指定消息的功能。在发送消息之前可以为消息设定指定的 Key，通常这个 Key 是在业务层面是唯一的：12Message msg = new Message("Topic", "Tag", "Content".getBytes());msg.setKey(uniqueKey); 尽管 Broker 不会对消息进行 Key 相关的过滤，但是会为消息定制相应的索引。看一下索引格式： Key HashCode：4 个字节的 Key 的哈希，用来快速检索 CommitLog Offset：8 个字节来保存 CommitLog 中的偏移量 Timestamp：使用 4 个字节记录消息存储时间和产生时间的时间差 Next Index Offset：使用 4 个字节来记录下一索引的偏移量在存储 Key 相应的索引时候，其实分了多个哈希桶来（Slot）存储，也就是相对 Key 进行了两次散列。怎么解决哈希冲突？因为索引结构中保存了 Key 的哈希，所以对于哈希值不同而模数相同的 Key 在查询时候可以直接区分开来。对于哈希值相等但是 Key 本身不相等的情况，客户端继续做一次 Key 比较来进行筛选。一般应用中进行消息过滤使用 Tag，而使用命令行工具 mqadmin 做运维时查询特定 Key 的消息，用法：1mqadmin queryMsgByKey -k &lt;Key&gt; -n &lt;NamesrvAddr&gt; -t &lt;Topic&gt; -f &lt;endTime&gt; 使用 MessageId 来查询消息每次消息成功发送后，都会生产一个 MsgId 和 OffsetMsgId，来标识这条消息：123456Message msg = new Message("Topic", "Tag", "Content".getBytes());SendResult result = producer.send(msg);// producer 产生的 idString msgId = result.getMsgId();// broker 产生的 idString offsetMsgId = result.getOffsetMsgId(); 对于 MsgId，由 producer ip + pid + MessageClientIDSetter.class.getClassLoader().hashCode() + time + counter 组成 而对于 OffsetMsgId，由 broker ip + CommitLog Offset 组成，可以精确地定位消息存储的位置 同时我们可以使用运维工具 mqadmin 针对 OffsetMsgId 进行检索1mqadmin queryMsgById -n &lt;NamesrvAddr&gt; -I &lt;OffsetMsgId&gt; 使用自定义属性和类 SQL 过滤在发送消息前，我们可以为消息设置自定义的属性：123Message msg = new Message("Topic", "Tag", "Content".getBytes());msg.putUserProperty("p1", "v1");msg.putUserProperty("p2", "v2"); 在服务端进行消费时候，可以针对自定义属性，利用类 SQL 的表达式来进行消息的进一步筛选：1consumer.subscribe("Topic", MessageSelector.bySql("p1 = v1"); 使用这样的方式进行过滤，需要 Broker 先从 CommitLog 中取出消息，得到消息中的自定义属性进行对应的计算。理所当然的，功能很强大，但是效率没有使用 Tag 的过滤方式高。 对于表达式的语法支持如下： 对比操作： 数字：&gt;, &lt;, &lt;=, &gt;=, =, BETWEEN 字符串：=, &lt;&gt;, IN 空值判断：IS NULL, IS NOT NULL 逻辑判断：AND, OR, NOT 数据类型： 数字：123，456 字符串：’abc’, ‘def’, 必须使用单引号 空值：NULL 布尔：TRUE，FALSE 使用自定义代码和 Filter Server对于 Filter Server，事实上实在 Broker 所在服务器启动了多个类似中转代理的进程，这几个进程负责充当 Consumer 从 Broker 上拉取代码，使用用户上传的 Java 代码进行过滤，最后传送给消费者。这个中转代理会和 Broker 本身争抢 CPU 资源，需要按需求谨慎使用；同时用于过滤的代码需要严格的审查，避免可能影响 Broker 宕机的风险操作。这个过滤操作只支持 PushConsumer使用流程： 启动 Broker 时指定 filterServerNums=&lt;n&gt;，当然使用配置文件也可以。n 的数量就是中转代理 FilterServer 的进程数 实现 org.apache.rocketmq.common.filter.MessageFilter 接口，定制过滤逻辑 接收消息：PushConsumer.subscribe(final String topic, final String fullClassName, final String filterClassSource) filterClassSource 是前一步 MessageFilter 接口实现的源码，必须使用 utf-8 编码。这会在 Consumer 启动时将过滤逻辑上传至 Broker 参考： MessageId 生成解读 https://www.cnblogs.com/linlinismine/p/9184917.html]]></content>
      <categories>
        <category>RocketMQ</category>
      </categories>
      <tags>
        <tag>MQ</tag>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ 日志设置]]></title>
    <url>%2F2018%2F08%2FRocketMQ-%E6%97%A5%E5%BF%97%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[日志配置文件位置RocketMQ 日志基于 slf4j 实现，支持 Logback、Log4j。如果需要指定日志的配置文件的位置有三种方式： 环境变量：ROCKETMQ_CLIENT_LOG_CONFIGFILE=&lt;custom-file&gt; 启动参数：rocketmq.client.log.configFile=&lt;customer-file&gt;，作为 JVM 变量，启动时时需要增加 -D 标识，优先级也比环境变量更高 作为 Java 实现，日志位置信息是通过 System.getProperty() 或者 System,getenv() 得到的，所以可以在程序入口 System.setProperty(“rocketmq.client.log.configFile”, customer_file) 来配置 日志相关系统变量 rocketmq.client.log.loadconfig默认 true，是否加载指定配置文件，当设置为 false 时，RocketMQ 客户端会会使用应用本身的日志配置。这可能反而是最简单的日志配置方式 rocketmq.client.log4j.resource.fileName、rocketmq.client.logback.resource.fileName、 rocketmq.client.log4j2.resource.fileName三种日志框架的的配置文件名，默认值分别为 log4j_rocketmq_client.xml、logback_rocketmq_client.xml、log4j2_rocketmq_client.xml rocketmq.client.log.configFile日志配置文件路径，上述。如果使用了自定义的日志配置文件，通常你不再需要设置以下的变量了 rocketmq.client.logRootRocketMQ 日志信息默认存放日志为：$USER_HOME/Logs/rocketmqLogs，通过改变此变量可以变更日志路径 rocketmq.client.logLevel日志输出级别，默认 INFO rocketmq.client.logFileMaxIndex滚动窗口的索引最大值，默认 10]]></content>
      <categories>
        <category>RocketMQ</category>
      </categories>
      <tags>
        <tag>MQ</tag>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ Producer 摘要]]></title>
    <url>%2F2018%2F08%2FRocketMQ-Producer-%E6%91%98%E8%A6%81%2F</url>
    <content type="text"><![CDATA[上一篇介绍完了 RocketMQ 消费者的默认实现，现在来瞅一瞅生产者的用法。 设置必要的属性同样的，是 DefaultMQProducer，新建实例之后，在使用生产者发送消息之前，需要初始化几个属性： InstanceName 实例名称这是为了当一个 JVM 上启动了多个生产者时，区分不同的生产者实例，系统默认名称为 DEFAULT RetryTimesWhenSendFailed 重试次数当消息投递失败时，有可能是因为网络原因，可以设置多投递几次减少丢消息的情况。很多实用者在使用时，为了避免重复的消息设置不重试是不正确的做法：因为 RocketMQ 本身并不保证消息的不重复，作为客户端对消息进行幂等处理是必要的。而在次前提下，对发送失败的场景拒绝重发，不仅对避免重复消息没有任何意义，同时也增加了消息的丢失的可能。 NamesrvAddr需要 NameServer 的地址，写法和 Consumer 一致 消息发送方式和投递结果发送方式 同步发送：Producer.send(Message message) 异步发送：Producer.send(Message message, SendCallback callback) 发送结果对于消息发送的结果，存在四中可能返回的状态。而且在不同的配置方式下，意义可能有所不同 SEND_OK发送成功，标志着消息已经成功被发送到 Broker。（这时候不一定意味着主从复制完成或者刷盘完成） FLUSH_DISK_TIMEOUT刷盘时间超时，只有在刷盘策略为 SYNC_FLUSH 时才可能出现 FLUSH_SLAVE_TIMEOUT主从同步时间超时，只有在主备形式下使用 SYNC_MASTER 才可能出现 SLAVE_NOT_AVAILABLE从机缺失，只有在主备形式下使用 SYNC_MASTER 才可能出现，类似于 FLUSH_SLAVE_TIMEOUT对于不同的业务场景具体需求，如何处理消息发送的结果是程序质量的一个重要考量点 特殊的消息延迟消息RocketMQ 支持延迟消息，Broker 收到消息后并不会立即投递，而是等待一段时间后再讲消息送出去。 使用方式：在消息发送前执行 Message.setDelayTimeLevel(int level) 延迟等级：默认 1s/5s/10s/30s/1m/2m/3m/4m/5m/6m/7m/8m/9m/10m/20m/30m/1h/2h，索引 1 开始尽管 RocketMQ 的延迟消息不支持任意精度，但是各等级的延迟是可以预设的，更改配置文件即可 队列选择对于一个 Topic 通常有多个 MessageQueue 来接收消息，默认情况下 Producer 轮流向各个 MessageQueue 发送消息，而 Consumer 根据默认的负载策略进行消费，所以无法明确对应 Producer 的消息是哪个 Consumer 消费。在需要指定特定 MessageQueue 来投递消息时，可以实现 MessageQueueSelector 接口，定制选择逻辑；发送时选择带有选择器的重载方法即可 事务消息介绍事务消息是必要的，但是并不推荐使用。因为事务消息会造成磁盘脏页，影响磁盘性能，在 4.x 版本中已经移除，需要使用时需要手动根据顶层接口实现。简单的说，RocketMQ 的事务消息流程如下： 向 Broker 发送消息（消息状态为未确认状态） Broker 对收到的消息完成持久化，返回成功状态。发送的第一阶段结束 执行本地逻辑 事务消息的结束 本地逻辑结束，客户端向 Broker 确认消息 commit：提交，该消息将会被 Broker 进行投递 rollback：回滚，Broker 会删除之前接收到的消息 超过一定时间，服务端对客户端发起回查请求Producer 对回查请求返回 commit 或者 rollback 的响应。如果此时发送消息的 Producer 无法访问，回查请求会发送给同一 ProducerGroup 内的其他 Producer 参考 RocketMQ on GitHub 《RocketMQ 实战与原理解析》机械工业出版社 杨开元]]></content>
      <categories>
        <category>RocketMQ</category>
      </categories>
      <tags>
        <tag>MQ</tag>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ Consumer 摘要]]></title>
    <url>%2F2018%2F08%2FRocketMQ-Consumer-%E6%91%98%E8%A6%81%2F</url>
    <content type="text"><![CDATA[结束了对 RocketMQ 组件的初步理解以及配置的简单设定，可以对 RocketMQ 仔细研究一番了。先来看看 RocketMQ 的消费者实现，以及服务端是如何处理消费者客户端的请求，把消息送出去的。 RocketMQ 对于消费者客户端，支持推模型和拉模型。对于推模型，由消息服务端作为主动方，向客户端推送消息（尽管其本质是一个长轮询式的拉模型实现）；而拉模型由客户端主动拉取消息。 PushConsumer客户端的实现：DefaultMQPushConsumerImpl 是客户端的一个默认实现，可以从 pullMessage() 方法切入，观察它的实现。 基本要素：以下几个属性，不仅仅是推模型的重要配置，同时也称得上是每个客户端的标配。 NameServerAddr指定 NameServer 地址是必要的，可以通过客户端 API 设置（使用 ; 分割多个地址），或者使用环境变量 NAMESRV_ADDR ConsumerGroup将多个消费者组织一起，提高并发，需要配合 MessageModel 属性一起使用 MessageModel消息模式分为两种，集群模式：Clustering；广播模式：Broadcasting Clustering：集群模式，所订阅 Topic 下的消息，每一条只会被同一 ConsumerGroup 下的一个消费者所消费，达到负载均衡的目的 Broadcasting：广播模式，同一 ConsumerGroup 下的每一个 Consumer 都会消费到所订阅 Topic 下的全部消息。 Topic消息类型主题，作为不同消息的标识，决定了消费者订阅哪些消息。Topic 默认是可以由客户端创建的，生产环境下通常改权限被关闭，需要使用 mqadmin 工具来初始化可用的 Topic TagTag 可以进一步过滤消费需要订阅的消息，在 Java 客户端 API 下，使用 null 或者 * 来消费所有 Tag 类型，需要具体指定时可以使用 || 来分割多个 Tag 服务端推送方式：消费者的推模型是通过长轮询实现的，因为完全的推模型方式会使得服务端增加许多压力，明显的降低效率，同时也会因为各客户端消费能力不足的问题造成隐患。Broker 服务端在处理客户端请求时如果发现没有消息，会休眠一小会-短轮询间隔（shortPollingTimeMills），重复循环，直到超过最大等待时间（brokerSuspendMaxTimeMills），在此期间内的收到消息会立即发送给客户端，达到“推”的效果 客户端流量控制：客户端维护了一个线程池来接受服务端“推”来的消息，针对每个 MessageQueue 都有使用一个 ProcessQueue 来保存快照状态和处理逻辑。ProcessQueue 主要由一个 TreeMap 和读写锁组成 ProcessQueue.lockTreeMap 保存了所有获取后还没有被消费的消息 Key：MessageQueue‘s offset Value：消息内容引用 DefaultMQPushConsumerImpl.pullMessage() 会检查以下每个属性，任意属性超过阈值会暂缓拉取动作。由于通过 ProcessQueue 的信息来比较，检查域是每个 Queue cachedMessageCount检查当前缓存的但是未消费的消息数量是否大于设定值（pullThresholdForQueue，默认 1000） cachedMessageSizeInMiB同上，检查队列中消息缓存的大小（pullThresholdSizeForQueue，默认 100MiB） maxSpan检查 ProcessQueue 中未消费消息的 offset 跨度（consumeConcurrentlyMaxSpan，默认 200），在顺序消费时不检查 PullConsumer客户端的实现：初次接触，可以从这几个方法了解 PullConsumer 的消息拉取思路，并从官方的几个例子中了解一些常用的处理方式。 前置操作 DefaultMQPullConsumerImpl.fetchSubscribeMessageQueues() DefaultMQPullConsumerImpl.fetchConsumerOffset() DefaultMQPullConsumerImpl.fetchMessageQueuesInBalance() 拉取动作 DefaultMQPullConsumerImpl.pull() DefaultMQPullConsumerImpl.pullBlockIfNotFound() 客户端额外操作：在使用 PullConsumer 时候，通常使用需要额外关心 MessageQueue 和 offset 等一些要素，灵活的封装可以带来更多的自主性。以 fetchSubscribeMessageQueues() 和 pull() 方法说明几个要素： MessageQueue一个 Topic 下通常会使用多个 MessageQueue，如果需要获取全部消息，需要遍历返回的所有队列。特殊情况下可以针对特定队列消费 Offsetstore使用者需要手动记录和操作消息偏移量，随着消息消费而改变它，需要额外注意他的持久化，正确的偏移量是准确消费的前提 PullStatus针对某队列的拉取动作结束，会返回相应状态，使用者需要针对不同状态采取不同的动作 FOUND NO_MATCHED_MSG NO_NEW_MSG OFFSET_ILLEGAL shutDown()关闭操作会进行保存 offset 的操作，在 NameServer 注销客户端的操作等。对于保存的 offset 可以通过 OffsetStore 对象获取，启动时加载。 参考 RocketMQ on GitHub 《RocketMQ 实战与原理解析》机械工业出版社 杨开元]]></content>
      <categories>
        <category>RocketMQ</category>
      </categories>
      <tags>
        <tag>MQ</tag>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RokcetMQ 配置项]]></title>
    <url>%2F2018%2F08%2FRocketMQ-%E9%85%8D%E7%BD%AE%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[RocketMQ 的配置分为两部分，一者是 JVM 的配合，另一者则是对 Broker 应用本身的参数配置。在初次接触时候，除了 RocketMQ 本身的一些特性，同时也难免会被一些配置给迷惑或者踩坑，这里来看一下通常的配置点。 Broker JVM 配置JVM 的配置默认不需要修改，只需要根据硬件情况调整相应的堆栈内存和对外内存的占用量即可。附上启动时的 JVM 配置脚本片段：123456789101112JAVA_OPT="$&#123;JAVA_OPT&#125; -server -Xms8g -Xmx8g -Xmn4g"JAVA_OPT="$&#123;JAVA_OPT&#125; -XX:+UseG1GC -XX:G1HeapRegionSize=16m -XX:G1ReservePercent=25 -XX:InitiatingHeapOccupancyPercent=30 -XX:SoftRefLRUPolicyMSPerMB=0 -XX:SurvivorRatio=8 -XX:+DisableExplicitGC"JAVA_OPT="$&#123;JAVA_OPT&#125; -verbose:gc -Xloggc:/dev/shm/mq_gc_%p.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintAdaptiveSizePolicy"JAVA_OPT="$&#123;JAVA_OPT&#125; -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=30m"JAVA_OPT="$&#123;JAVA_OPT&#125; -XX:-OmitStackTraceInFastThrow"JAVA_OPT="$&#123;JAVA_OPT&#125; -XX:+AlwaysPreTouch"JAVA_OPT="$&#123;JAVA_OPT&#125; -XX:MaxDirectMemorySize=15g"JAVA_OPT="$&#123;JAVA_OPT&#125; -XX:-UseLargePages -XX:-UseBiasedLocking"JAVA_OPT="$&#123;JAVA_OPT&#125; -Djava.ext.dirs=$&#123;BASE_DIR&#125;/lib"#JAVA_OPT="$&#123;JAVA_OPT&#125; -Xdebug -Xrunjdwp:transport=dt_socket,address=9555,server=y,suspend=n"JAVA_OPT="$&#123;JAVA_OPT&#125; $&#123;JAVA_OPT_EXT&#125;"JAVA_OPT="$&#123;JAVA_OPT&#125; -cp $&#123;CLASSPATH&#125;" 需要额外关注的点在于： -Xms8g -Xmx8g -Xmn4g 默认 Broker 需要 8g 的堆内存，不要轻易在自己的笔记本上运行哦 😂 -XX:MaxDirectMemorySize=15g 默认的最大堆外内存为 15g，nio 通过内存映射文件所提高 IO 效率而用。 JAVA_OPT_EXT 该环境变量可以追加和替换原有的配置 Broker 应用配置自定义配置启动启动 Broker 时可以自定义配置：sh bin/mqbroker -c CONFIG.properties 配置可选项 获取可配置项的列表：sh bin/mqbroker -m 获取配置项以及默认值：sh bin/mqbroker -p 源码中配置类：BrokerConfig / NettyServerConfig / NettyClientConfig / MessageStoreConfig 配置参数介绍介绍几个常用的，或者说通常需要配置的选项。 namesrvAddr=IP:PORT;IP:PORT配置 NameServer 的地址，多个地址间使用 ; 隔开，该选项没有默认值，可以启动时通过 -n 参数设置 brokerClusterName=DefaultCluster配置 RocketMQ 集群的名称，默认为 DefaultCluster brokerName=broker-aBroker 的名称，在同一 NameServer 群下，只有使用相同的 brokerName 的 Broker 实例才可以组成主从关系 brokerId=0在一个 Broker 群下（都使用了同样的 brokerName），所有实例通过 brokerId 来区分主从，主机只有一个：brokerId=0（默认） fileReservedTime=48消息数据在磁盘上保存的时间，单位：小时，默认：48 deleteWhen=04在指定的时间删除那些超过了保存期限的消息，标识小时数，默认：凌晨 4 时 brokerRole=SYNC_MASTER有三种选项，前两者主要描述 Broker 实例间的同步机制 SYNC_MASTERBroker Master 的选项，消息同步给 Slave 之后才返回发送成功状态 ASYNC_MASTERBroker Master 的选项，主从间消息同步异步处理 SLAVEBroker Slave 的选项（没得选） flushDiskType=ASYNC_FLUSH有两种选项，分别同步或异步的刷盘策略 SYNC_FLUSH消息只有在真正写入磁盘之后才会返回成功状态，牺牲性能，但可以确保不丢失消息 ASYNC_FLUSH异步刷盘，消息写入 page_cache 后即返回成功 brokerIP1=127.0.0.1设置 Broker 对外暴露的 IP，通常 Broker 启动时会自动探测，但是由于容器环境或者多网卡的影响，通常需要手动设置。需要多个暴露 IP 时，可以使用 brokerIP2/3/4/... 的方式配置 listenPort=10911Broker 实例监听的端口号 storePathRootDir=/home/rocketmq/store-a存储消息和一些配置的根目录 参考 RocketMQ on GitHub 《RocketMQ 实战与原理解析》机械工业出版社 杨开元]]></content>
      <categories>
        <category>RocketMQ</category>
      </categories>
      <tags>
        <tag>MQ</tag>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ 配置探索]]></title>
    <url>%2F2018%2F07%2FRocketMQ-%E9%85%8D%E7%BD%AE%E6%8E%A2%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[目前被广泛使用的 MQ 有很多，包括 ActiveMQ，Kafka，RabbitMQ，RocketMQ 等等，它们各有长短。而近期所在项目选择了 RocketMQ 作为消息中间件，此前并未系统地了解研究，所以趁此机会整理了一些笔记和想法。 优势简单地说一下在这么多消息中间件中的选型优势。作为阿里的开源项目，想必还是可靠的，尤其是经受过双十一的考验令人信服。 支持严格的消息顺序； 支持 Topic 与 Queue 两种模式； 亿级消息堆积能力； 比较友好的分布式特性； 同时支持 Push 与 Pull 方式消费消息； 基本概念 Producer：消息生产者，生产消息。 Consumer：消息消费者，消费消息。 Pull Consumer：消费者拉模型的实现。通过与 Broker 建立长连接，从中主动拉取消息。 Push Consumer：消费者推模型的实现。本质仍然是建立长连接，但是通过注册监听器，在收到消息时回调监听方法。 Producer Group：生产者集合，通常包含发送逻辑一致的消费者，影响事务消息的流程。 Consumer Group：消费者集合，通常包含消费逻辑一致的消费者，影响着负载均衡和集群消息。 Name Server：注册服务器，可以由一到多个近乎无状态的节点构成，扮演者类似 Zookeeper 的角色。Broker 向其中注册，而 Producer 和 Consumer 向其中拉取 Broker 地址。 Broker：核心组件，保存和转发消息。 拓扑结构如下： 初次使用下载RocketMQ 是纯 Java 语言的实现，你可以从 Github 上下载源码并使用 Maven 进行编译，当然也可以从官网入口下载。 启动第一次启动，简单地测试一下效果，进入 bin 目录，使用 nohup 启动一下 NameService： nohup ./mqnamesrv -n 127.0.0.1:9876 &amp; 然后启动一下 Broker： nohup ./mqbroker -n 127.0.0.1:9876 &amp; 还有一个 mqadmin 也是常用的工具，包含的管理员常用的功能，包含查看集群列表，查看、删除主题等，可以直接通过 ./mqadmin 获得帮助。 测试在 RocketMQ 顺利启动之后，进行一下测试吧，快速的体验一把。从 MavenRepository 找到对应的 RocketMQ 客户端：123456&lt;!-- https://mvnrepository.com/artifact/org.apache.rocketmq/rocketmq-client --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-client&lt;/artifactId&gt; &lt;version&gt;4.2.0&lt;/version&gt;&lt;/dependency&gt; 或者打开刚才从 Github 上下载的源码，example 模块下提供了许多测试用例，附上略微改动的生产者和消费者代码： 生产者 123456789101112131415161718192021222324252627282930public static void main(String[] args) throws MQClientException, InterruptedException &#123; DefaultMQProducer producer = new DefaultMQProducer("ProducerGroupName"); producer.setNamesrvAddr("127.0.0.1:9876"); producer.setInstanceName("p001"); // 可以设定失败重试次数 producer.setRetryTimesWhenSendFailed(3); producer.start(); for (int i = 0; i &lt; 1; i++) &#123; try &#123; Message msg = new Message( "TopicTest1", "TagA", "key113", "Hello world".getBytes(RemotingHelper.DEFAULT_CHARSET)); SendResult sendResult = producer.send(msg); System.out.printf("%s%n", sendResult); QueryResult queryMessage = producer.queryMessage("TopicTest1", "key113", 10, 0, System.currentTimeMillis()); for (MessageExt m : queryMessage.getMessageList()) &#123; System.out.printf("%s%n", m); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; producer.shutdown();&#125; 消费者 1234567891011121314151617181920212223242526public static void main(String[] args) throws InterruptedException, MQClientException &#123; DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("ConsumerGroupName"); // 指定 NameServer 的地址，多个 NameServer 使用 ; 隔开 consumer.setNamesrvAddr("127.0.0.1:9876"); consumer.setInstanceName("c001"); // 指定订阅的 Topic 以及 Tag，多个 Tag 使用 || 分开，* 代表全部 Tag consumer.subscribe("TopicATest1", "TagA"); // 可以设定开始消费的位置，仅针对 Push Consumer consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); // 可以设定批量消费数量，默认 1，不保证每次的数量，近针对 Push Consumer consumer.setConsumeMessageBatchMaxSize(1); consumer.registerMessageListener(new MessageListenerConcurrently() &#123; @Override public ConsumeConcurrentlyStatus consumeMessage( List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; for (MessageExt msg : msgs) &#123; System.out.println(new String(msg.getBody())); &#125; return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); consumer.start(); System.out.println("Consumer Started.");&#125; 动手运行一下吧。 关于配置事实上，大多数小伙伴在 RocketMQ 启动时都明显能感觉电脑卡卡的，是因为 RocketMQ 默认需求的内存太大了。那么，如何查看和修订所需要的配置呢？之前我们通过 ./mqbroker 启动了 Broker，那么来看一下 mqbroker 的脚本，注意脚本末尾的命令：12# 省略 ROCKETMQ_HOME 的配置sh $&#123;ROCKETMQ_HOME&#125;/bin/runbroker.sh org.apache.rocketmq.broker.BrokerStartup $@ 这里将启动命令转移给了 runbroker.sh 进行执行。 JVM 参数配置既然如此，继续查看一下 runbroker.sh：12345678910111213141516171819202122232425262728293031323334353637383940414243444546#!/bin/sh#===========================================================================================# Java Environment Setting#===========================================================================================error_exit ()&#123; echo "ERROR: $1 !!" exit 1&#125;[ ! -e "$JAVA_HOME/bin/java" ] &amp;&amp; JAVA_HOME=$HOME/jdk/java[ ! -e "$JAVA_HOME/bin/java" ] &amp;&amp; JAVA_HOME=/usr/java[ ! -e "$JAVA_HOME/bin/java" ] &amp;&amp; error_exit "Please set the JAVA_HOME variable in your environment, We need java(x64)!"export JAVA_HOMEexport JAVA="$JAVA_HOME/bin/java"export BASE_DIR=$(dirname $0)/..export CLASSPATH=.:$&#123;BASE_DIR&#125;/conf:$&#123;CLASSPATH&#125;#===========================================================================================# JVM Configuration#===========================================================================================JAVA_OPT="$&#123;JAVA_OPT&#125; -server -Xms8g -Xmx8g -Xmn4g"JAVA_OPT="$&#123;JAVA_OPT&#125; -XX:+UseG1GC -XX:G1HeapRegionSize=16m -XX:G1ReservePercent=25 -XX:InitiatingHeapOccupancyPercent=30 -XX:SoftRefLRUPolicyMSPerMB=0 -XX:SurvivorRatio=8 -XX:+DisableExplicitGC"JAVA_OPT="$&#123;JAVA_OPT&#125; -verbose:gc -Xloggc:/dev/shm/mq_gc_%p.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintAdaptiveSizePolicy"JAVA_OPT="$&#123;JAVA_OPT&#125; -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=30m"JAVA_OPT="$&#123;JAVA_OPT&#125; -XX:-OmitStackTraceInFastThrow"JAVA_OPT="$&#123;JAVA_OPT&#125; -XX:+AlwaysPreTouch"JAVA_OPT="$&#123;JAVA_OPT&#125; -XX:MaxDirectMemorySize=15g"JAVA_OPT="$&#123;JAVA_OPT&#125; -XX:-UseLargePages -XX:-UseBiasedLocking"JAVA_OPT="$&#123;JAVA_OPT&#125; -Djava.ext.dirs=$&#123;BASE_DIR&#125;/lib"#JAVA_OPT="$&#123;JAVA_OPT&#125; -Xdebug -Xrunjdwp:transport=dt_socket,address=9555,server=y,suspend=n"JAVA_OPT="$&#123;JAVA_OPT&#125; $&#123;JAVA_OPT_EXT&#125;"JAVA_OPT="$&#123;JAVA_OPT&#125; -cp $&#123;CLASSPATH&#125;"numactl --interleave=all pwd &gt; /dev/null 2&gt;&amp;1if [ $? -eq 0 ]then if [ -z "$RMQ_NUMA_NODE" ] ; then numactl --interleave=all $JAVA $&#123;JAVA_OPT&#125; $@ else numactl --cpunodebind=$RMQ_NUMA_NODE --membind=$RMQ_NUMA_NODE $JAVA $&#123;JAVA_OPT&#125; $@ fielse $JAVA $&#123;JAVA_OPT&#125; $@fi 通过这个文件可以获得很多信息： RocketMQ 的 JVM 配置信息 需求的内存空间达到了 8g，声明的最大堆外内存达到了 15g，这就是电脑变得卡卡的的原因了。 可以在启动时配置 JAVA_OPT_EXT 变量来配置额外的参数或者覆盖默认配置。 结合 mqbroker.sh 可以发现，最终使用了 BrokerStartup 来启动 RocketMQ，命令行中的参数同时会被传递。 Broker 实例配置那么接下来就去 BrokerStartup 查看一下 RocketMQ 的启动过程。由于这个文件实在是太过冗长，这里不再贴出，感兴趣的小伙伴请自行查看。在这个文件中，主要对命令行中几个具体参数进行了解析： -m：列出所有的配置项 -p：列出所有的配置项以及默认值 -c：指定一个 properties 文件，读取其中的内容覆盖默认配置并情动 自定义配置所以，很多时候的做法是通过 sh mqbroker -p &gt; mqbroker.properties 来获得一份默认配置文件（网上的方案可能不太准确，具体输出是携带 Rocket 的日志信息的，需要 sed 或者 awk 之类加工处理一下），在此基础上进行配置自定义，然后通后通过 sh mqbroker -c mqbroker.properties 来进行定制化的启动。 默认配置方案同时在 conf 目录下，官方也给出了几种典型的配置方案供参考： 二主二从异步复制：2m-2s-async 文件夹。这是最典型的生产配置，双 master 获得高可用性，同时主从间的数据同步由异步完成。 二主二从同步复制：2m-2s-sync 文件夹。除了双 master 的配置，主从间的数据是同步的，也就是说只有在向 salve 成功同步数据才会向客户段返回成功。这保证了在 master 宕机时候消息仍然可以被实时消费，但是性能收到一定影响。 二主无从：2m-nosalve 文件夹。双主模式仅仅保证了 RocketMQ 的高可用性，然而在一台 master 宕机后，客户端无法消费那批在宕机 master 上持久化的消息，直到宕机 master 恢复正常。当然这个方案节省了硬件资源。 三种默认配置方案都是采用了异步刷盘，尽管在刷盘间隙宕机会丢失少量数据，但是效率提升可观。 参考配置类Broker 的具体配置分为了具体的四个方面： Broker 实例配置：参考源码 org.apache.rocketmq.common.BrokerConfig Netty 服务端配置：参考源码 org.apache.rocketmq.remoting.netty.NettyServerConfig Netty 客户端配置：参考源码 org.apache.rocketmq.remoting.netty.NettyClientConfig Message 持久化配置：参考源码 org.apache.rocketmq.store.config.MessageStoreConfig 关于 RocketMQ 的启动和配置，就先告一段落。]]></content>
      <categories>
        <category>RocketMQ</category>
      </categories>
      <tags>
        <tag>MQ</tag>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现 MyBatis 插件]]></title>
    <url>%2F2018%2F07%2F%E5%AE%9E%E7%8E%B0-MyBatis-%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[MyBatis 作为一个目前很常用的持久化框架，有着丰富的拓展。这些拓展功能常常以插件的形式嵌入到 MyBatis 的运作流程之中，而如何制作实现一个插件？MyBatis 已经为大家设计好了，一个 Interceptor 接口，实现它就够了。 Interceptor 接口的拦截目标，是 MyBatis 运作流程中的几个核心组件： Executor：这是 MyBatis 执行器，控制着所有和数据库交互的操作，也影响着一级缓存。 ParameterHandler：参数处理器，在映射参数时候生效。 ResultSetHandler：结果集处理器，在处理结果集的时候会用到。 StatementHandler：Executor 下层的处理器，同样控制着 SQL 行为，也控制着二级缓存的生效。 这几个组件就简称处理器对象吧，感兴趣的话，可以跟进资料，这里继续来讲插件如何拦截它们以及如何实现一个插件。 Interceptor 接口Interceptor 接口是插件的核心，看一下它的接口：12345678public interface Interceptor &#123; // 拦截后的逻辑 Object intercept(Invocation invocation) throws Throwable; // 将处理器对象包装成代理类 Object plugin(Object target); // 初始化属性赋值 void setProperties(Properties properties);&#125; intercept()：拦截 MyBatis 的执行过程，需要在其中加入定制的逻辑。 plugin()：可以理解为插件的构造过程，通常把 MyBatis 的几个 handler 包装成代理用。 setProperties()：用于插件初始化时候的属性赋值。如果你有其他的赋值方案，也可以不采用它。 我们从第一个方法开始讲起。 Object intercept(Invocation invocation)入参 Invocation 是一个 MyBatis 封装的对象，包含了运行时的信息： 属性Method method：即反射包中的 Method，在这里它是当前运行的方法。 属性Object[] args：方法的参数列表 属性Object target：这里其实是你选择拦截的处理器对象（关于如何选择拦截具体的处理器对象，稍后再述），也就是说，它可以是 Executor / StatementHandler …，需要使用时可以直接强转。 方法 proceed()：让处理器继续流程，或者调用下一个插件，你可以用 Filter.doFilter() 来类比它。 MyBatis 插件是通过动态代理实现的，对处理器对象进行代理，由代理对象在方法 invoke() 前完成插件中 interceptor() 方法（即插件逻辑）。同时多个插件又是多层的代理，每个插件都需要在具体方法调用前完成自己的逻辑，所以在实现 Interceptor 接口的 intercept 方法最后，一定要记得执行 Invocation.proceed()，以完成插件的调用链：1234567@Overridepublic Object intercept(Invocation invocation) throws Throwable &#123; // 可以通过 invocation 获得处理器对象，进而可以变更参数，埋点，收集信息等 // do something // 最后需要记得完成调用链，否则流程将中段 return invocation.proceed();&#125; Object Plugin(Object)该方法在处理器对象初始化的时候，由 InterceptorChain.pluginAll() 调用，将处理器对象包装成代理类。可以理解为一个初始化方法。 以 StatementHandler 举例：1234567891011121314public StatementHandler newStatementHandler(Executor executor, MappedStatement mappedStatement, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) &#123; StatementHandler statementHandler = new RoutingStatementHandler(executor, mappedStatement, parameterObject, rowBounds, resultHandler, boundSql); // 初始时触发代理包装 statementHandler = (StatementHandler) interceptorChain.pluginAll(statementHandler); return statementHandler;&#125;public Object pluginAll(Object target) &#123; // 迭代完成所有插件代理，最终返回一个包含所有插件逻辑的处理器对象代理 for (Interceptor interceptor : interceptors) &#123; target = interceptor.plugin(target); &#125; return target;&#125; 该方法的本质目的是使得新的代理类在拦截的目标方法以及之前的插件逻辑之前添加上新插件的 intercept() 方法中的内容。所以该方法 Object 类型的入参与出参自然也就是处理器接口对象了。在没有特殊需求的情况下，推荐使用官方工具类 Plugin.wrap() 方法来完成：1234@Overridepublic Object plugin(Object target) &#123; return Plugin.wrap(target, this);&#125; 原因嘛…先来看一下 Plugin.wrap()：123456789101112131415161718192021222324252627282930public static Object wrap(Object target, Interceptor interceptor) &#123; // 插件上都通过 @Interceptors 指定了要拦截的处理器，以及要拦截的方法和参数，收集起来 // 获得这个插件想拦截的类-方法 Map&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt; signatureMap = getSignatureMap(interceptor); // 这个 type 必然是 4 大执行器/处理器 接口实现之一 Class&lt;?&gt; type = target.getClass(); // 获得原来的所实现的接口，动态代理的必要步骤 Class&lt;?&gt;[] interfaces = getAllInterfaces(type, signatureMap); // 如果该插件没有拦截这个处理器，在上一个方法会返回空数组，这里就不包装了 if (interfaces.length &gt; 0) &#123; return Proxy.newProxyInstance( type.getClassLoader(), interfaces, new Plugin(target, interceptor, signatureMap)); &#125; return target;&#125;@Overridepublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; try &#123; Set&lt;Method&gt; methods = signatureMap.get(method.getDeclaringClass()); if (methods != null &amp;&amp; methods.contains(method)) &#123; return interceptor.intercept(new Invocation(target, method, args)); &#125; return method.invoke(target, args); &#125; catch (Exception e) &#123; throw ExceptionUtil.unwrapThrowable(e); &#125;&#125; 好处在于，不在需要开发者手动构建一个动态代理（Plugin 本身就是一个 InvocationHandler 实现类），并且在包装成代理的时候，将四个处理器中不需要拦截的类排除了，这使得运行中减少一层不必要的代理，进而提升效率。 @Intercepts 注解插件的拦截流程都已经明了，回过来梳理一下如何拦截自己想要的指定的处理器和指定的方法呢？ 在实现了 Interceptor 接口之后，需要配合 @Intercpts 注解一起使用。这个注解中需要安置一个 Signature 对象，在其中指定你需要指定： type：选择 4 个处理器类之一。 method：选择了处理器之后，你需要选择拦截那些方法。 args：选择拦截的方法的参数列表。因为如 Executor 中 query 方法是有重载的。 通过以上三者，插件便确定了拦截哪个处理器的哪个方法。MyBatis 的插件实现是不是很简单呢？ 需要注意的是，Exector 和 StatementHandler 在一些功能上类似，但是会影响不同级别的缓存，需要注意。同时由于 sqlSession 中这 4 个处理器对象的功能着实强大，并且可以通过拦截改变整个 SQL 的行为，所以如果需要深入定制插件行为的时候，最好需要对 MyBatis 核心机制由一定的了解。 官方介绍http://www.mybatis.org/mybatis-3/zh/configuration.html#plugins]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 挂载与数据存储]]></title>
    <url>%2F2018%2F07%2FDocker%2F</url>
    <content type="text"><![CDATA[Docker 镜像是层层分离的，分为只读的底层和可读写的当前层。容器在运行时候如果有文件改动，会自动从含有该文件的底层拷贝并更新在当前层。如果容器在 commit 之前被销毁，那么有这个镜像重新生成的容器是不包含改动的内容的。 需求来源所以数据问题是使用 Docker 必然会关注到的，除了如何持久化以外，如何从宿主机访问到容器内的数据？或者将容器内的数据同步到宿主机？又或是多个容器间怎么共享数据？这都是要处理的。 还好 Docker 提供了一套完善而简单的数据挂载机制 Volume。 命名卷要控制容器的数据内容，首先从文件的挂载开始。docker volume 提供了一套管理卷（volume）的 API：12345678910Usage: docker volume COMMANDManage volumesCommands: create Create a volume inspect Display detailed information on one or more volumes ls List volumes prune Remove all unused local volumes rm Remove one or more volumes 先创建一个 named volume：docker volume create vol使用 docker volume ls 可以看到当前存在的所有的 volume。使用 docker volume rm 删除指定的 volume。使用 docker volume inspect vol 可以看到它的详情，包括创建时间和宿主机上的真正挂载位置等： [ { &quot;CreatedAt&quot;: &quot;2018-07-09T14:53:05Z&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: {}, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/vol/_data&quot;, &quot;Name&quot;: &quot;vol&quot;, &quot;Options&quot;: {}, &quot;Scope&quot;: &quot;local&quot; } ] 可以看到这个新建的 vol 保存在 /var/lib/docker/volumes/vol/_data 下，其中 /var/lib/docker/volumes 目录保存了所有的 Docker volume。 运行时挂载使用命名卷有了 volume 之后，我们便可以使用刚才创建的 vol 来挂载容器中的某个目录：1docker run -d -v vol:/data --name temp-redis redis 如此一来，在 temp-redis 容器中 /data 下的改动，都会同步反映在宿主机的 /var/lib/docker/volumes/vol/_data 下；而宿主机的改动亦然。 使用绝对路径使用命名的 volume 通常是为了数据共享，很多时候我们只是想指定一个挂载的目录便于记忆和管理，这个时候可以使用绝对路径，如：1docker run -d -v /data/docker_volume/temp_redis/data:/data --name temp-redis redis 自动挂载有时候你甚至不想关心任何 volume 的形式，你只是想把某个目录持久化而已，可以这么做：1docker run -d -v /data --name temp-redis redis Docker 会自动生成一个匿名的 volume。想要知道具体的宿主机目录可以使用：docker inspect 来查看。这种情况通常在构建纯数据容器时使用。 注意点在 volume 创建时和创建之后仍然需要关注他们，下面是一些典型的问题。 volume 自动创建事实上在 -v vol:/data 时候，vol volume 甚至不需要提前使用 docker volume create vol 创建，使用 docker run -v vol:/data 命令时便会自动创建。同时地，也意味着 -v 选项不支持相对路径的使用。 volume 的删除在一个 volume 被创建之后，想删除可没那么容易，即使使用了 docker rm CONTAINER 删除了容器，volume 依然保留着。除非： 使用 docker run --rm 启动的容器停止时。它除了会删除容器本身还会删除挂载的匿名 volume。 使用 docker rm -v v 参数可以删除容器和创建容器时关联的匿名 volume。 那么我们在使用了 named volume 或者删除容器时忘记了 -v，那么那些在 /var/lib/docker/volumes 下的一些文件就成了僵尸文件。怎么删除呢？ 使用 docker volume rm VOLUME 来删除。 使用 docker volume prune 删除所有不再被使用的 volume。 volume 的只读控制一些场合下，我们提供只是需要容器内的程序去读取一些内容而非修改。我们可以严格的控制 volume，增加只读选项 ro：1234docker run -d \ --name=nginxtest \ -v nginx-vol:/usr/share/nginx/html:ro \ nginx:latest 通过 Dockerfile 挂载可以通过 Dockerfile 在构建镜像的时候便指定需要的 volume，这对于很多应用都是必要的，尤其是一些数据类应用。Dockerfile 中使用 VOLUME 表明挂载目录，如：1VOLUME ["/data1", "/data2"] 任何通过该镜像构建的容器都会将 /data1，/data2 两个目录进行挂载。但是 Dockerfile 形式的弱势是无法进行 named volume 或者绝对路径的挂载。 数据共享与存储共享 volume既然数据可以在宿主机和容器间同步，那么可以使多个容器间同步吗？当然可以！ –volumes-from1234# 首先创建一个容器，并挂载 /datadocker run -d -v /data --name ng1 nginx# 创建第二个容器，共享前者的 volumedocker run -d --volumes-from ng1 --name gn2 nginx named volume1234# 首先创建一个容器，并创建命名卷 share 来挂载 /datadocker run -d -v share:/data --name ng1 nginx# 创建第二个容器，使用同样的 /datadocker run -d -v share:/data --name ng2 nginx 两种方式都能达到数据共享的目的，但是由于通过命名卷的方式对多个容器的依赖进行了解耦，所以推荐第二种。 数据容器这个话题事实上和数据共享紧密相关，由于在 Docker1.9 之前，大家广泛使用使用 --volumes-from 的形式来共享卷，导致必须要一个底层的没有依赖的容器来保存数据。 通常使用和应用容器一样的镜像来构建数据容器 数据容器不需要也不应该启动，仅仅是利用 volume 机制来保持数据而已 然而现在有了命名卷，完全不需要数据容器的存在了，使用 named volume 可以更直接更方便的管理共享数据。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile 中三个运行指令的差异]]></title>
    <url>%2F2018%2F07%2FDockerfile-%E4%B8%AD%E4%B8%89%E4%B8%AA%E8%BF%90%E8%A1%8C%E6%8C%87%E4%BB%A4%E7%9A%84%E5%B7%AE%E5%BC%82%2F</url>
    <content type="text"><![CDATA[在描述 Dockerfile 的时候，对于 RUN，CMD，ENTRYPOINT 三个命令，用法十分相似，功能也差不多，容易让人混用。其实一般来说，三个命令都能完成需要的操作，而差异点常常被一些使用者忽略。这里简单说一下，三个命令的不同之处。 命令格式首先看一下 Dockerfile 中如何执行命令。在 Dockerfile 中，命令（instruction）一般有两种写法，分别是： Shell 格式：INSTRUCTION &lt;command&gt; &lt;option&gt; &lt;param&gt; Exec 格式：INSTRUCTION [&quot;command&quot;, &quot;option&quot;, &quot;param&quot;] 两个格式基本没有差异，除了可读性之外，对于 Shell 格式的命令，Docker 会自动使用 /bin/bash -c 来进行解析，可以是解析命令中的变量比如 $JAVA_HOME。而如果使用 Exec 格式执行时需要解析环境变量，需要进行修改，比如：CMD [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo&quot;, &quot;java home is $JAVA_HOME&quot;]。对于 RUN，CMD，ENTRYPOINT 三者同样遵守此规则。 RUN 命令RUN 命令在 Dockerfile 中可以多次使用，所以通常被用来在构建容器时执行一些必须的前置命令，比如安装软件等。它的出现频率远高于其他两个执行命令。格式： RUN &lt;command&gt; &lt;option&gt; &lt;param&gt; RUN [&quot;command&quot;, &quot;option&quot;, &quot;param&quot;] 带来一个 Shell 风格的安装 Git 的例子：1RUN apt-get update &amp;&amp; apt-get install -y git 这里使用 &amp;&amp; 可以使得在同一层镜像层上更新 apt 并下载 git。 CMD 命令CMD 命令的格式有三种，前两者都是用来定义容器的默认行为，后者用来给 ENTRYPOINT 命令提供额外的可替换参数。 CMD &lt;command&gt; &lt;option&gt; &lt;param&gt; CMD [&quot;command&quot;, &quot;option&quot;, &quot;param&quot;] CMD [&quot;param&quot;...] 先说前两者用作执行默认命令的情况，以一个官方 Nginx 的 Dockerfile 为例：12# 前置步骤忽略CMD ["nginx", "-g", "daemon off;"] 该命令使得以该 Nginx 镜像构建的容器，在启动时候默认地自动运行 Nginx。但是既然是定义默认行为，那么它是可以在运行容器的时候被更改的，比如像下面这样：1sudo docker run -p 80:80 nginx echo hello-world 那么这个时候容器并不会启动一个 Nginx 服务，而是打印了 hello-world。并且这个容器会随着打印命令的结束而停止。 需要注意的是，既然 CMD 定义默认行为，那么它在 Dockerfile 中只能存在一个（如果定义了多个 CMD，那个最后一个有效） 那么如何使用 CMD 为 ENTRYPOINT 提供额外参数呢？先看一下 ENTRYPOINT 的用法。 ENTRYPOINT 命令ENTRYPOINT 通常用来定义容器启动时候的行为，有点类似于 CMD，但是它不会被启动命令中的参数覆盖。上一节中 Nginx 的例子，如果将 CMD 改成 ENTRYPOINT，那么我们的 hello-world 方案便行不通了。 ENTRYPOINT 同样支持两种格式的写法，并且是存在差异的（后文描述）： ENTRYPOINT &lt;command&gt; &lt;option&gt; &lt;param&gt; ENTRYPOINT [&quot;command&quot;, &quot;option&quot;, &quot;param&quot;] 使用 Exec 风格的写法支持使用 CMD 拓展可变参数和动态替换参数，而使用 Shell 风格时不支持。假如我们在 Dockerfile 中：12# 前置步骤忽略ENTRYPOINT redis-cli -h 127.0.0.1 那么这个由此构建的容器在运行时只能将 redi 连接到容器本身的 redis-server 上。修改这个 Dockerfile：123# 前置步骤忽略ENTRYPOINT ["redis-cli"]CMD ["-h", "127.0.0.1"] 这时候，如果按默认的启动方式，容器的 redis-cli 会自动连接 127.0.0.1，即以下命令此时是等价的：12docker run my-redis-imagedocker run my-redis-image -h 127.0.0.1 但是由于 Dockfile 中使用了 Exec 格式的 ENTRYPOINT，我们已经可以修改它的目标地址了，甚至可以增加额外的参数：1docker run my-redis-image -h server.address -a redis_password 其中 -h server.address 替换了 CMD [&quot;-h&quot;, &quot;127.0.0.1&quot;]，而 -a redis_password 则是由于使用了 Exec 格式可以增加参数。 关于 RUN，CMD，ENTRYPOINT 的差异已经描述完了，有没有都牢记于心呢~]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文字处理-sed]]></title>
    <url>%2F2018%2F07%2F%E6%96%87%E5%AD%97%E5%A4%84%E7%90%86-sed%2F</url>
    <content type="text"><![CDATA[sed 是非常常用的流式处理编辑工具，配合正则表达式可以进行十分效率的文字处理。它通过逐行地将文字读取，暂存进模式空间，通过指定的命令进行处理后，输出至标准输出，直到文件或者说输入的结束。原理十分简单，来学习一下吧。 命令格式sed [选项及参数]… ‘操作命令’ 文件… 常用选项 -n 静默模式 取消 sed 默认的输出 -e 多重编辑 可以多次使用，携带 sed 命令作为该选项的参数，对于缓冲区的每一行按命令顺序进行处理 -f 使用 sed 操作的脚本处理，跟上脚本文件名做参数。sed 脚本可以保存多条 sed 命令，一行一条，依次执行。 -r 使用拓展正则，类似 grep -e 或者 egrep 的增强，让人少些一点转义符 -i 原地修改，该选项会使得 sed 命令直接修改原文件，可以紧跟一个后缀如 -i.bk，同时生成备份 命令组成sed 命令的组成通常是 ‘行选择具体操作’。sed 在读入行内容之后，如果匹配定址要求就会进行命令操作。我们来两部分分开一看~ 行选择 num 选择第 num 行。 start,end 选择第 start 行至第 end 行。 start~step 从 start 行开始，以 step 为步长选择行。 /pattern/ 选择含有该正则的内容的行。 start,/pattern/ 从 start 行开始直到首次成功匹配表达式的一行将被选定，注意 sed 的处理机制，如果表达式最终无法有任何匹配行，将会对 start 行之后的所有行进行选定。 /pattern/,end 从首次成功匹配表达式的一行至 end 行将被选定，如果在第 end 行之前如果没有成功匹配将会不有行被选中。 ! 置于行选择末尾，进行反选，如 10,20! 将排除 10 至 20 行。 具体操作 p 打印缓冲内容，通常配合 -n 选项测试指令正确性。 q 提前退出 sed，当到达匹配行或者指定行就退出 sed。 i\[content] 行前追加内容，如在第 1 行至第 5 行，每行之后插入‘===’：&#39;1,5i\===&#39;。 a\[content] 行后追加内容。 c\[content] 替换内容，如果是批量选择，并不是对每行内容进行替换，而是对整体替换成相应内容。 d 删除。 s/pattern/new_chars/[g] 对选定的行中首次匹配表达式的内容替换成目标字符串，如果末尾使用‘g’，可以对行中所有匹配内容替换。 y/old-char-set/new-char-set 对选定行中所有 old-char-set 内的字符替换成相应的新的字符。 n 提前读取下一输入行至缓冲区。 r [file-name] 行后追加指定文件内容。 w [file-name] 将选定行输入至指定文件。 h 复制匹配行进入暂存空间。 G 配合 h 命令，取出暂存空间内的行，插入至模式空间的末尾。 x 配合 h 命令，交换当前匹配行的暂存空间内的行。 命令叠加需要多次使用 sed 操作的时候，可以有这么些办法： 通过 {命令1;命令2...} 可以使用‘{}’将多个命令整合一块儿，中间使用‘;’分割，类似代码块的表达。其中如果如果多个命令的行定位条件一致，可以将该部分提出，如：&#39;{10p;10p}&#39; 等于 &#39;10{p;p}&#39;。 通过 -e 选项多次输入 sed 命令。 将多个 sed 命令逐行写入文件，使用 -f 选项执行 sed。 额外技巧sed 命令其实和正则表达式通常配合使用，所以这一些小技巧其实和正则有很密切的关系。比如对于一个简单的替换操作，如 s/abc/xyz/ 是十分简单的，但是 sed 支持更复杂的操作，可以用一些特殊意义的操作符号: &amp; 表示正则匹配成功的原字符串内容，如想要替换所有数字为它的百倍，可以使用 ‘s/[[:digit:]]\+/&amp;00/g’ \num 获取缓存的匹配中缓存的字符串，num 指定位置（这取决于表达式中使用了多少次小括号‘()’）。 如去除数字并字母大写，’s/([[:digit:]]\+\)\|\([[:alpha:]]\+\)/\U\2/gp’ 获取包含一段 ip 的文本中 ip 的前两段地址 ‘s/\([0-9]\+\):\([0-9]\+\):[0-9]\+:[0-9]+/\1 \2/p’ \u 将后方的表达式结果进行首字母大写处理 \U 将后方的表达式结果进行大写处理 \l 将后方的表达式结果进行首字母小写处理 \L 将后方的表达式结果进行小写处理]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>Regex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AQS 浅析]]></title>
    <url>%2F2018%2F07%2FAQS-%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[java.util.concurrent 包中提供了许多易用而有效的工具类，诸如 ReentrantLock，CountDownLatch，Semaphore 等等，给日常的并发编程带来了许多便利。他们都使用了同样的框架来完成基本的同步过程：AbstractQueuedSynchronizer （AQS）来实现基本功能，比如获取资源和释放的步骤。 简单了解戳开 AQS 一览其结构，其实它本身维护了两个概念： state：（volatile int）该属性维护了资源的状态，或者是数量。 CLH queue：一个先进先出的线程等待队列。这并不是一个具体对象，而是通过内部类 Node 来维护的。 AQS 对于 state 支持两种模式的资源共享形式： Exclusive-排他式：进程独占形式，如 ReentrantLock，Mutex 的应用。 Share-共享式：支持多线程访问，典型的应用式 CountDownLatch / Semaphore。 子类实现 AQS 时候只需要实现关于 state 的获取（acquire）和释放（release）方案即可，包括队列的维护，线程的唤醒等工作，大部分都在 AQS 维护了。举个栗子，在使用 CountDownLatch 的时候，我们会初始化一个计数值 n 用于对应 n 个子线程，这个 n 同时也对应了 state 值，每一次 countDown() 的时候，会使得 state CAS 地减 1。在 state 归零的时候会使用 unpark()，主线程 从 await() 函数返回。 工作流程AQS 的 API 同一般的同步工具 API 一样，除了对于资源的 acquire / release 操作，还提供的了 tryAcquire / tryRelease 的非阻塞操作。同时 acquireInterruptibly 支持线程中断。如果需要使用共享式的操作，需要实现对应的 Share 操作方法。 资源获取首先看 acquire 方法：1234public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; tryAcquire(int)：尝试获取资源 addWaiter(Node)：使线程（Node 对象维护这线程对象）进入等待队列，对于 acquire 方法使用 EXCLUSICE-排他模式。 acquireQueued(Node, int)：在这一步骤中线程会等待，而在线程唤醒后会尝试在该方法内获取资源。 selfInterrupt：由于线程在等待过程中无法响应中断，所以在获取资源并退出队列后补充一个中断。 tryAcquire(int) 方法默认会抛出操作不支持的异常，需要子类的具体实现。 123protected boolean tryAcquire(int arg) &#123; throw new UnsupportedOperationException();&#125; addWaiter(Node, int) 方法会自旋使用 CAS 方式将一个 Node 加入队尾。 1234567891011121314151617181920212223242526272829private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; enq(node); return node;&#125;private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; 从上面两个方法可以看出位于队列头部的 Node 其实只是一个标记，在队列第二位置的时候，线程已经可以获取资源并进行相关任务了。 acquireQueued(Node, int) 更关键的一步，在获取资源失败， Node 已经被加入队尾之后，线程需要进入等待状态等待被唤醒。 123456789101112131415161718192021final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; shouldParkAfterFailedAcquire(Node, Node) 每个节点是否需要等待需要阻塞取决于前驱节点的状态。 1234567891011121314private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; if (ws == Node.SIGNAL) return true; if (ws &gt; 0) &#123; do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; 前驱节点最后会通过该状态值来判断是否需要 unpark 下个线程。在这里，如果前驱节点标识为 SIGNAL，则进入等待；标识为 CANCAL 需要追溯更前的节点状态；如果为其他正常值，则更新为 SIGNAL。 parkAndCheckInterrupt() 使线程进入 WATING，等待 unpark（在 release 中触发，马上就来） 或者 interrupt，被唤醒后回到 acquireQueued 触发中断或者继续检查是否可以获取资源。 1234private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; 释放资源并唤醒后置线程这是 acquire 方法的反操作，用于资源的释放，当资源成功释放时，唤醒下一个线程（位于头节点之后）。 123456789public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; 通过 tryRelease(int) 方法判断资源是否释放，这个方法同样需要被实现： 123protected boolean tryRelease(int arg) &#123; throw new UnsupportedOperationException();&#125; 而 unparkSuccessor(Node) 方法用来真正唤醒 node.next 中的线程： 123456789101112131415private void unparkSuccessor(Node node) &#123; int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) LockSupport.unpark(s.thread);&#125; 看到这就能了解整个 AQS 的运作流程了。在 parkAndCheckInterrupt 中进入 WAITING 的线程，在这里被唤醒，它会继续进入 acquireQueued 中的自旋，如果 tryAcquire 顺利获得资源，则将本线程的节点设置为 head 并返回 acquire 方法。so, go on. AQS 本身并不复杂，使用时只需要手动实现 tryAcquire 和 tryRealeas 方法。 而对于 Share-共享式的 acquire / release 流程，区别并不太大，有兴趣的小伙伴可以自行翻阅源码一探究竟。 参考 https://mp.weixin.qq.com/s/eyZyzk8ZzjwzZYN4a4H5YA]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ribbon 摘要]]></title>
    <url>%2F2018%2F07%2FRibbon%2F</url>
    <content type="text"><![CDATA[主要构成Ribbon 是由 netflix 开源的一个客户端负载均衡组件。从客户端的角度取维护服务间请求的负载均衡，并进行一定的容错处理。自然的它的核心接口就是：com.netflix.loadbalancer.ILoadBalancer。 在配置 Ribbon 之前，了解一下 Ribbon 完成负载均衡的几个重要组成部分： IRule：负载均衡的策略。 IPing：检测服务存活的策略。 ServerList&lt;T&gt;：拉取服务实例列表的策略。 ServerListUpdater：更新服务列表的触发策略。 ServerListFilter&lt;T&gt;：服务过滤方案。 参考下面的类关系图，可以有一个更好的印象： IRule负载均衡策略接口，可能最常需要配置的就是它了，配置也没什么特殊的，就以 Spring 常用的方式即可。通常情况下 Ribbon 原生的几个负载均衡策略应该可以满足生产要求（当然你也可以自定义），来了解一下： AbstractLoadBalancerRule 顶级的抽象类，给予了获得 LoadBalancer 的方法，可以让子类获得负载均衡器的一些信息，定制更具体的算法。 RandomRule 通过 LoadBalancer 获取可用的服务实例，并随机挑选。（一直无可实例时有无限循环 bug） RoundRobinRule 通过 LoadBalancer 获取可用的服务实例，并轮询选择。（超过 10 次失败时，打印机警告并返回 null） RetryRule 默认有一个 RoundRobinRule 的子规则，和 500 毫秒的阈值。使用子规则选择实例，执行时间若超过阈值则返回 null。 WeightedResponseTimeRule 继承自 RoundRobinRule。在构造时会启动一个定时任务，默认每 30 秒执行一次，来计算服务实例的权重。在默认的算法下，响应速度越快的服务实例权重越大，越容易被选择。 ClientConfigEnabledRoundRobinRule 本身定义了一个 RoundRobinRule 的子规则。本且默认的 choose 方法也是执行 RoundRobinRule 的实现。本身没有特殊用处，这个默认会实现是在其子类的算法无法实现时采用，通常会选择该类作父类继承，实现自定义的规则，以保证拥有一个默认的轮询规则。 BestAvaliableRule 通过 LoadBalancerStats 选择并发请求最小的实例。 PredicateBasedRule 利用子类的 Predicate 过滤部分服务实例后通过轮询选择。 AvailabilityFilteringRule 轮询选择一个服务实例，判断是否故障（断路器断开），并发请求是否大于阈值（默认2^32-1，可通过&lt;clientName&gt;.&lt;nameSpace&gt;.ActiveConnectionsLimit 修改）。允许则返回，不允许则再次选择，失败 10 次后执行父类方案。 ZoneAvoidanceRule 使用组合过滤条件执行过滤，每次过滤后会判断实例数是否小于最小实例数（默认1），是否大于过滤百分比（默认0），不再过滤后使用轮询选择。 具体配置在项目中是可以配置多个 Ribbon 客户端的，通常来说每个客户端用来访问不同的服务。比如为访问 A 服务的 Ribbon 客户端配置为 A-Client，B 服务为 B-Client。 通过代码配置首先来介绍通过注解配置的方法，像简单的 Sring Bean 配置一样，不过不需要使用 @Configuration 注解了。在配置类上启用 @RibbonClient，给定客户端的名称和配置类，使用 @Bean 来配置具体的组件如 IRule 等。 123456789101112131415@RibbonClient(name = "A-Client",configuration = ARibbonConfig.class)public class ARibbonConfig &#123; // 服务实例的地址 String listOfServers = "http://127.0.0.1:8081,http://127.0.0.1:8082"; @Bean public ServerList&lt;Server&gt; ribbonServerList() &#123; List&lt;Server&gt; list = Lists.newArrayList(); if (!Strings.isNullOrEmpty(listOfServers)) &#123; for (String s: listOfServers.split(",")) &#123; list.add(new Server(s.trim())); &#125; &#125; return new StaticServerList&lt;Server&gt;(list); &#125;&#125; 官方文档上提示了一个坑，不能把加了配置注解的具体的配置类放在 @ComponentScan 路径下，否则先扫描到的一个具体的客户端配置会成为 Ribbon 的全局配置。 这怎么能忍？当然得有更优雅的解决方式：全局配置的方案。使用 @RibbonClients 注解，一次可以描述多个客户端配置类的位置，同时也可以指定默认配置类，如： 12345678910@SpringCloudApplication@RibbonClients(value = &#123; @RibbonClient(name = "A-Client",configuration = ARibbonConfig.class), @RibbonClient(name = "B-Client",configuration = BRibbonConfig.class)&#125;, defaultConfiguration = DefaultConfig.class)public class DemoServiceApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(DemoServiceApplication.class, args); &#125;&#125; 这样配置可以不需要再在配置类上加上注解了，也可以不需要将配置类移出包扫描路径。 通过配置文件指定但是以上这样的代码配置还是稍显复杂，在目前的 SpringCloud 中 Ribbon 的配置可以直接 SpringBoot 的配置文件中写入，使用如下的方式指定要配置的参数： &lt;nameSpace&gt;.&lt;key&gt;=&lt;value&gt; 默认的命名空间是 ribbon，例如定义连接超时时间可以： ribbon.connectTimeout=120 而当需要为具体的客户端配置时，可以使用： &lt;client&gt;.&lt;nameSpace&gt;.&lt;key&gt;=&lt;value&gt; 比如： user-service.ribbon.listOfServers=localhost:8001,localhost:8002 关于 ribbon 所有的参数名称，可以参看 com.netfix.client.config.CommonClientConfigKey&lt;T&gt;。 与 Eureka 整合后的配置在使用 Eureka 的时候，会改变 Ribbon 一些组件的默认实现，如： ServerList -&gt; DiscoveryEnabledNIWSServerList：由 Eureka 来维护服务列表 IPing -&gt; NIWSDiscoveryPing：由 Eureka 测试服务存活（原配的 DummyPing 并不会 ping，而是始终返回 true） 而且针对不同服务不需要显示地配置不一样的客户端名称了，只需要使用 &lt;serviceName&gt;.&lt;nameSpace&gt;.&lt;key&gt;=&lt;value&gt; 如 user-service： user-service.ribbon.ReadTimeout=120 同时由于 SpringCloud Ribbon 默认实现区域亲和策略，zone 的配置也十分简单，只需要加入元数据集中即可，如： eureka.instance.metadataMap.zone=huzhou 如果不需要 Eureka 辅助 Ribbon 的自动配置（这不太可能吧），则可以使用： ribbon.eureka.enable=false 这时候，记得自己得手动配置 listOfServers 等参数了。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringCloud</tag>
        <tag>Ribbon</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则笔记]]></title>
    <url>%2F2018%2F06%2F%E6%AD%A3%E5%88%99%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[日常开发中，经常需要进行一些文本处理，这通常是十分繁琐而无趣的。学会并利用正则表达式可以快速解决这类文本处理问题，无论是在 Java，Python 等代码中抑或是 shell 环境下。正则中存在很多细小的知识点，十分容易遗忘，着手记录，知识整理还是有所必要。 本文以拓展正则进行描述，部分特殊字符和定址方式在标准正则下无效。如需在 shell 环境下使用，需要额外使用‘\’符号，或开启拓展正则的支持，如 grep -e，sed -r 等。 规则正则表达式的规则可以简单理解为：在给定的范围内，给定的字符重复给定的次数。 指定需要匹配的字符绝大部分的字符，包括数字字母汉字等，都可以直接输入来描述。 转义后特殊字符以下几个字符由转义符‘\’和某个字符组成，可以表述成新字符，他们有着特殊的含义。 \n 这是一个换行符。 \t 制表符，Tab 键的缩进。 \v 垂直制表符。 \f 分页符，产生一个新页。 \r 这是回车符 \s 这个表达可以表述所有的空白字符，即以上五种字符或者空格。 \cx 当 x 取值英文字符时，这个整体就有了特殊意义，会映射成一个控制字符，如 \cM 等价于 \r 即回车符号。 \ux 可以匹配 Unicode 字符，如 \u00CD。 \nx 在 x 值合法的情况下，可以匹配八进制专一值（在没有顺利取得缓存引用时） \b 表示字符边界，可以理解为打断单词或着汉字连续的空格/符号之类（其实它并不能匹配到某个字符，仅仅是匹配到了边界） \d 表示数字，同 [0-9]。 \w 表示任意字类字符，即数字字母和下划线，通常还支持汉字。 \&lt; 匹配单词首，比如 \&lt;wo 可以匹配 wood，word 等。 \&gt; 同理匹配单词尾部。 原生特殊字符以下的字符并不需要转移符‘\’，天然的拥有特殊含义（以拓展正则为准）。 $ 尾部，表示字符串结尾位置。 ^ 头部，字符串的开头位置，但在 [ ] 内使用时表示取反 [ ] 左右中括号，用来表达匹配单个字符时候的可选范围 { } 左右花括号，用来表述前一表达式的可重复区间 ( ) 左右小括号，类似数学中的概念，可以描述一个表达式并提高计算优先级，同时会缓存匹配结果。 · 点号，可以用了匹配任意的一个字符，除了 \n 吧。 * 星号，可以匹配前面表达式任意次数。 + 加号，匹配前面表达式一至任意次数. ? 问号，匹配前面表达式 0 ~ 1 次。 \ 转移符本身，用来转移其他字符，需要匹配它本身的时候自然需要 \\ 的形式。 | 或运算符号，任意匹配前后表达式之一。 [:digit:] 所有数字 [:lower:] 所有小写字母 [:upper:] 所有大写字母 [:alpha:] 所有字母 [:alnum:] 所有字母及数字 [:space:] 所有空白符 [:punct:] 所有标点符号 指定匹配字符的候选范围匹配一个单字符很简单，但是通常我们需要匹配几个字符中的任意一个，这个时候就需要一个候选范围的描述。除了使用 \w 表示字类字符，\s 来表示空白符。也可以使用 [ ] 方括号来描述范围，来看几个例子： [abc] 它可以是匹配 a，也可以是 b，也可以是 c。 5[48]k 它可以是 54k，也可以是 58k。 [0-9]\w 它可以使是 0a，3b，33 等等。 这样一看是不是很容易？来看一些进阶技巧： 取反。表述范围的符号可以通过大写来表示取反，\S 表示非空白字符，\B 表示非字符边界，\W 表示非字类字符。对于使用 [ ] 的场合，使用 [^ ] 来完成取反。 [ ] 方括号中间不再能使用上述特殊的字符，比如 [x*]，* 不再匹配任意次 x，这个表达式只能匹配 * 或 x；同理比如：[\s] 表示 \ 或者 s。 [ ] 中可以使用一些特定的范围，比如 0-9，a-z，A-Z。比如式子 [0-9A-Z] 也是合理的，会匹配数字或者大写字母，如需要匹配‘-’，尽量写在最后。 指定表达式的重复次数在需要重复一个特定的正则表达式的时候，我们可以使用限定符描述重复次数来简化它。 {n} 匹配 n 次，如 C{3} 即 CCC。 {n,} 匹配大于等于 n 次。如 C{1,} 等同于 C+ {n,m} 匹配大于等于 n 次，小于等于 m 次，闭区间。 * 等同于 {0,} + 等同于 {1,} ? 等同于 {0,1} 利用正则匹配的缓存 \num 使用 ( ) 之后会进行匹配值的缓存，可以紧跟着 \num 指定匹配的子项，比如 (.)\1 可以匹配任意两个相同的字符。或者在 sed，vim 等工具使用替换操作时，可以在新字符串上使用 \num 以期达到精确的匹配但是局部替换的效果。 (?:pattern) 使用该符号会取消匹配值的缓存 (?=pattern) 正向肯定预查，使用该符号会取消匹配值的缓存，同时预查也是不消耗字符的。 (?!pattern) 反向预查，相反于前者]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>Regex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟机中的锁膨胀]]></title>
    <url>%2F2018%2F06%2F%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B8%AD%E7%9A%84%E9%94%81%E8%86%A8%E8%83%80%2F</url>
    <content type="text"><![CDATA[尽管当下几乎所有的服务器环境都以集群为主，在考虑并发问题的时候通常会使用分布式技术,如 redis 等中间件来维护全局的资源和锁。但是对于一些实例层面上的资源，依旧需要使用传统的锁来维护。所以我觉得，理解 JVM 对锁的处理还是有价值的。 JVM上针对锁的处理（这里只描述内部锁，即 synchronized 的处理情况），除了有自旋锁，锁粗化，锁消除等简单的自动优化机制（不探讨啦），还可以从锁的维护的角度去看，可以分为偏向锁，轻量级锁，重量级锁。三者的开销是递增的，演变顺序也是递增的，而且不可逆。如轻型的锁可以膨胀升级至重型锁，但是不可以从重型的锁降级。 之所以有不同程度的锁的处理方式，可以看一下这一个场景：如果在逻辑上某一时间基本只有一个线程，会访问由某个锁对象控制的同步块，很多时候不存在资源征用的问题。那么在这个时候，很多时候上锁解锁的开销就显得繁琐而低效了。以重量级锁（也是 synchronized 的最终形态）为例，取锁过程需要操作系统的介入，使线程从用户态进入核心态，开销还是挺大的。所以一开始 JVM 会对锁做偏向锁处理，在一些条件下升至轻量级锁乃至重量级锁。 前置概念在阐述锁膨胀之前，先插入几个内容点，对象头和自旋锁。 对象头在 HotSpot 虚拟机中，对象在内存中存储的布局可以分为三块区域：对象头（Header）、实例数据（Instance Data）和对齐填充（Padding）。 实例数据内容和对齐填充不在赘述，说说对象头，这就类似一个对象的元信息，其中也分为三部分组成：标记字段 Mark Word，类型指针 Class Pointer，数组长度 Array Length（如果该对象是数组，则记录了数组长度），三组数据分别用一个字长来存储（32位机上为32bit，64位机位64bit）。 类型指针：在虚拟机加载类的时候，除了会在元空间/方法区存储类信息，也会在堆区生成一个 Class 对象。类型指针，便是为一个实例对象指向 Class 对象的指针。 标记字段：在这一个字的区域内，描述了很多信息。通常一个对象会有其哈希码，年代标记（经历 GC 次数），锁标识等。但是如果该对象充当了一个上锁对象，情况会有所不同，下文详述。 自旋锁通常来说，取锁时候如果未能获得锁，线程会进入阻塞状态（Blocking），同时会让线程进入等待队列/同步块的入口集中，并导致一个上下文切换，出让 CPU 资源。我们可以实现一个忙轮询的机制来尝试获得锁。比如使用一个状态对象来代替锁，使用一个循环来判断状态是否未可用，如下是代码层面的实现。1234567volatile boolean flag = false;while(true) &#123; if(flag) &#123; // do something break; &#125;&#125; 这可以避免在首次取得锁失败的时候直接线程切出，在同步逻辑处理量较少的时候可以带来明显的效率提升，但是如果如果说“同步块”的处理时间很长，或者在“同步块”内的线程发生异常未能更改状态量，将严重损失性能乃至发生更严重的死锁。 所以自旋锁适合同步逻辑的处理时间很短的场合（几个循环内能拿到锁） 锁膨胀过程中的三个阶段现在已经了解了对象在堆中的存储形式，以及依靠自旋可以在短时间内减少加锁开销。继续深入 JVM 中不同并发程度下锁的不同的机制。 偏向锁偏向锁是在 JDK 1.5？1.6 之后对锁进行的优化。先来看一个图： 前文提到过，如果一个线程经常获得锁，且资源争用不严重，那么可以尽量减少取锁的开销。JDK 是这么做的：在一个线程获得锁的时候，会在栈帧记录中以及锁对象的标记字段中写入线程 id，在该线程进入/离开同步块的时候不需要额外的锁开销，这里甚至不需要 CAS 操作，因为只需要比较标记字段中的线程 id 与自身是否一致。如果在尝试获得锁的时候发现标记字段中的线程 id 与自身不一致，会尝试利用 CAS 操作来争抢这个偏向锁。 补充：偏向锁是默认启用的，但是它在应用程序启动几秒钟之后才激活，如有必要可以使用 JVM 参数来关闭延迟 -XX:BiasedLockingStartupDelay=0。如果你确定自己应用程序里所有的锁通常情况下处于竞争状态，甚至可以通过 JVM 参数关闭偏向锁 -XX:-UseBiasedLocking=false，那么所有的内部锁都会直接进入轻量级锁状态。 轻量级锁线程在执行同步块之前，JVM 会先在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的标记字段复制到锁记录中。然后线程尝试使用 CAS 将对象头中的标记字段替换为指向锁记录的指针。如果成功，当前线程获得锁；如果失败，则进行自旋来获取锁，当自旋获取锁仍然失败时，表示竞争严重（两条或两条以上的线程竞争同一个锁），则轻量级锁会膨胀成重量级锁。 重量级锁重量级锁其实才是通常涉及的锁概念，这时候它已经是一个彻底的悲观锁了。在 JVM 中又叫对象监视器，它至少包含一个竞争锁的队列，和一个信号阻塞队列（wait队列），前者负责做互斥，后一个用于做线程同步。 参考 《Java多线程编程实战指南》 黄文海 《深入理解Java虚拟机》 周志明 https://www.cnblogs.com/wade-luffy/p/5969418.html https://blog.csdn.net/wolegequdidiao/article/details/45116141]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>并发</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
</search>
